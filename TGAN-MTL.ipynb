{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9e5048-4bba-43d9-b132-6541c1625350",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "class DefaultConfig(object):\n",
    "    model = 'GTRAE_MVPT_MTL'\n",
    "    use_gpu = True\n",
    "    if use_gpu:\n",
    "        device = 'gpu'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "    load_model_path = None\n",
    "    data_name = 'iris'\n",
    "    batch_size = 128\n",
    "    num_workers = 0\n",
    "    max_epoch = 5000\n",
    "    patience = patience_acc = patience_mae = 1000 \n",
    "    lr = 0.001\n",
    "    lr_decay = 0.97\n",
    "    weight_decay = 1e-5\n",
    "    train_rate = 0.6\n",
    "    val_rate = 0.1\n",
    "    droput = 0.1\n",
    "    miss_rate = 0.05\n",
    "    whiten_rate = 0.1 \n",
    "    id_num = 1 \n",
    "    TOPK = 5 \n",
    "    n_hidden = 30 \n",
    "    use_all_to_train = True \n",
    "    model_save_path_acc = 'zz_saved_model/best_model_acc.pth'\n",
    "    model_save_path_mae = 'zz_saved_model/best_model_mae.pth'\n",
    "opt = DefaultConfig()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device=\", device)\n",
    "print(\"opt\",opt.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74924929-416a-4fd2-a5ad-c9d30fd22123",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import random\n",
    "import copy\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import xlwt, xlrd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils import data as torch_data\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score\n",
    "\n",
    "def calculate_imputation_metrics(imputed_data, target_data):\n",
    "    mae = mean_absolute_error(target_data.cpu().numpy(), imputed_data.cpu().numpy())\n",
    "    rmse = np.sqrt(mean_squared_error(target_data.cpu().numpy(), imputed_data.cpu().numpy()))\n",
    "    mape = np.mean(np.abs((target_data.cpu().numpy() - imputed_data.cpu().numpy()) / (target_data.cpu().numpy()+1e-8))) * 100\n",
    "    return mae, rmse, mape\n",
    "\n",
    "\n",
    "def calculate_imputation_metrics_mask(imputed_data, target_data, mask): \n",
    "    imputed_data_np = imputed_data.cpu().numpy()\n",
    "    target_data_np = target_data.cpu().numpy()\n",
    "    mask_np = mask.cpu().numpy()\n",
    "    filtered_imputed_data = imputed_data_np[mask_np == 0]\n",
    "    filtered_target_data = target_data_np[mask_np == 0]\n",
    "    mae = mean_absolute_error(filtered_target_data, filtered_imputed_data)\n",
    "    rmse = np.sqrt(mean_squared_error(filtered_target_data, filtered_imputed_data))\n",
    "    epsilon = 1e-8\n",
    "    mape = np.mean(np.abs((filtered_target_data - filtered_imputed_data) / (filtered_target_data + epsilon))) * 100\n",
    "    return mae, rmse, mape\n",
    "\n",
    "def calculate_accuracy(estimated_label, target_labels):\n",
    "    acc = accuracy_score(target_labels.cpu().numpy(), estimated_label.cpu().numpy())\n",
    "    return acc\n",
    "\n",
    "def writeline_csv(filename, rmse, mae, mape, acc, norm_rmse, norm_mae, norm_mape):\n",
    "    file_exists = os.path.isfile(filename)\n",
    "    with open(filename, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        if not file_exists:\n",
    "            writer.writerow(['Dataset','Miss Rate', 'RMSE', 'MAE', 'MAPE', 'Acc', 'Norm RMSE', 'Norm MAE', 'Norm MAPE'])\n",
    "        writer.writerow([opt.data_name, opt.miss_rate, rmse, mae, mape, acc, norm_rmse, norm_mae, norm_mape])\n",
    "\n",
    "def whiten(tensor_,whiten_rate = 0.05):\n",
    "    tensor = tensor_.clone()\n",
    "    attr = tensor.shape[1] // 3  \n",
    "    mask = tensor[:, :attr]  \n",
    "    valid_indices = torch.where(mask == 1)\n",
    "    num_to_whiten = int(whiten_rate * len(valid_indices[0]))\n",
    "    if num_to_whiten > 0:\n",
    "        indices_to_whiten = torch.randperm(len(valid_indices[0]))[:num_to_whiten]\n",
    "        mask[valid_indices[0][indices_to_whiten], valid_indices[1][indices_to_whiten]] = 0\n",
    "    tensor[:, :attr] = mask\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286ec801-0072-4962-9038-574cb4b1f3b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "class BasicModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicModule, self).__init__()\n",
    "        self.model_name = str(type(self))\n",
    "    def load(self, path):\n",
    "        self.load_state_dict(torch.load(path))\n",
    "    def save(self, name=None):\n",
    "        if name is None:\n",
    "            prefix = './checkpoints/' + self.model_name\n",
    "        torch.save(self.state_dict(), prefix)\n",
    "        return name\n",
    "    \n",
    "class GTRAE_MVPT_MTL(BasicModule):\n",
    "    def __init__(self, hidden_dim, num_classes, global_mask_shape, data_class, model_type):\n",
    "        super(GTRAE_MVPT_MTL, self).__init__()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        input_feature_dim = global_mask_shape[1]\n",
    "        self.model_name = 'GTRAE_MVPT_MTL' \n",
    "        self.dropout_layer = nn.Dropout(p=opt.droput) \n",
    "        self.relu = nn.ReLU() \n",
    "        self.graph_sage_layer1 = Graphsage(input_feature_dim, hidden_dim) \n",
    "        self.graph_sage_layer2 = Graphsage_hidden(hidden_dim, hidden_dim) \n",
    "        self.feature_transform = nn.Linear(hidden_dim, input_feature_dim) \n",
    "        self.class_transform = nn.Linear(hidden_dim, num_classes) \n",
    "        self.global_missing_values = nn.Parameter(torch.rand(global_mask_shape))\n",
    "        self.data_class = data_class\n",
    "        if model_type == 'train':\n",
    "            self.global_norm_miss_data = self.data_class.missing_data_train_norm.clone()\n",
    "            self.global_mask = self.data_class.mask_train.clone()\n",
    "            self.global_adj = self.data_class.adj_train.clone()\n",
    "        elif model_type == 'val':\n",
    "            self.global_norm_miss_data = self.data_class.missing_data_val_norm.clone()\n",
    "            self.global_mask = self.data_class.mask_val.clone()\n",
    "            self.global_adj = self.data_class.adj_val.clone()\n",
    "        elif model_type == 'test':  \n",
    "            self.global_norm_miss_data = self.data_class.missing_data_test_norm.clone() # X\n",
    "            self.global_mask = self.data_class.mask_test.clone() # M\n",
    "            self.global_adj = self.data_class.adj_test.clone() # A\n",
    "        elif model_type == 'all':  \n",
    "            self.global_norm_miss_data = self.data_class.missing_data_all_norm.clone() # X\n",
    "            self.global_mask = self.data_class.mask_all.clone() # M\n",
    "            self.global_adj = self.data_class.adj_all.clone() # A\n",
    "        self.global_norm_miss_data = torch.nan_to_num(self.global_norm_miss_data, nan=0.0)\n",
    "        aggregated_AX = torch.mm(self.global_adj, self.global_norm_miss_data) \n",
    "        aggregated_AAX = aggregated_AX + torch.mm(self.global_adj, aggregated_AX)\n",
    "        aggregated_AAAX = aggregated_AAX + torch.mm(self.global_adj, aggregated_AAX)\n",
    "        valid_weights_AX = torch.mm(self.global_adj, self.global_mask) \n",
    "        valid_weights_AAX = valid_weights_AX + torch.mm(self.global_adj, valid_weights_AX)\n",
    "        valid_weights_AAAX = valid_weights_AAX + torch.mm(self.global_adj, valid_weights_AAX)\n",
    "        self.global_aggregated_values = torch.where(valid_weights_AAAX > 0.2, aggregated_AAAX / (valid_weights_AAAX + 1e-8), torch.zeros_like(valid_weights_AAAX))\n",
    "    \n",
    "    def forward(self, norm_missing_data, batch_idx):\n",
    "        batch_mask = ~torch.isnan(norm_missing_data).to(self.device)\n",
    "        batch_missing_values = self.global_missing_values[batch_idx]\n",
    "        input_data = torch.where(batch_mask.bool(), norm_missing_data, batch_missing_values)\n",
    "        batch_aggregated_values = self.global_aggregated_values[batch_idx]\n",
    "        hidden_states_1 = self.graph_sage_layer1(input_data, batch_aggregated_values)\n",
    "        hidden_states_2 = self.graph_sage_layer2(hidden_states_1)\n",
    "        concatenated_result = torch.tensor([], device=self.device)\n",
    "        for j, hidden in enumerate(hidden_states_2[:-1]):\n",
    "            transformed_data_j = self.feature_transform(hidden)\n",
    "            feature_j = transformed_data_j[:, j].unsqueeze(1) \n",
    "            concatenated_result = torch.cat((concatenated_result, feature_j), dim=1) \n",
    "        class_logits = self.class_transform(hidden_states_2[-1])\n",
    "        imputed_data = concatenated_result\n",
    "        estimated_label = F.log_softmax(class_logits, dim=1)\n",
    "\n",
    "        return input_data, imputed_data, estimated_label\n",
    "\n",
    "class Graphsage(nn.Module): \n",
    "    def __init__(self, input_feature_dim, output_feature_dim):\n",
    "        super(Graphsage, self).__init__()\n",
    "        self.in_features = input_feature_dim\n",
    "        self.model_name = 'Graphsage'\n",
    "        self.W = nn.Parameter(torch.zeros(size=(2 * input_feature_dim, output_feature_dim)))\n",
    "        self.bias = nn.Parameter(torch.zeros(output_feature_dim))\n",
    "        self.reset_parameters()\n",
    "        self.bn = nn.BatchNorm1d(output_feature_dim)  \n",
    "        \n",
    "    def reset_parameters(self): \n",
    "        stdv = 1. / (math.sqrt(self.W.size(1)) + 1e-8)\n",
    "        self.W.data.uniform_(-stdv, stdv)\n",
    "        self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input_data, aggregated_data):\n",
    "        hidden_states = []  \n",
    "        n_attributes = input_data.size(1)  \n",
    "        concatenated_features = torch.cat([input_data, aggregated_data], dim=1)\n",
    "        for j in range(n_attributes): \n",
    "            modified_features = concatenated_features.clone()\n",
    "            modified_features[:, j] = 0\n",
    "            transformed_features = torch.mm(modified_features, self.W) + self.bias\n",
    "            hidden_states.append(transformed_features)\n",
    "        hidden_states.append(torch.mm(concatenated_features, self.W) + self.bias)\n",
    "        return hidden_states\n",
    "\n",
    "class Graphsage_hidden(nn.Module): \n",
    "    def __init__(self, hidden_feature_dim, output_feature_dim):\n",
    "        super(Graphsage_hidden, self).__init__()\n",
    "        self.model_name = 'Graphsage_hidden'\n",
    "        self.W = nn.Parameter(torch.zeros(size=(hidden_feature_dim, output_feature_dim)))\n",
    "        self.bias = nn.Parameter(torch.zeros(output_feature_dim))\n",
    "        self.reset_parameters()\n",
    "        self.bn = nn.BatchNorm1d(output_feature_dim) \n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.W.size(1))\n",
    "        self.W.data.uniform_(-stdv, stdv)\n",
    "        self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        new_hidden_states = []\n",
    "        for hidden in hidden_states:\n",
    "            transformed_hidden = torch.mm(hidden, self.W) + self.bias \n",
    "            new_hidden_states.append(transformed_hidden)\n",
    "        return new_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164677f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data as torch_data\n",
    "import numpy as np\n",
    "\n",
    "class Dataload_zz(torch_data.Dataset):\n",
    "    def __init__(self, norm_missing_data, norm_full_data, labels_onehot,adj_matrix):\n",
    "        self.norm_missing_data = norm_missing_data \n",
    "        self.norm_full_data = norm_full_data\n",
    "        self.labels_onehot = labels_onehot \n",
    "        self.adj_matrix = adj_matrix\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.norm_missing_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_norm_missing_data_sample = self.norm_missing_data[idx]\n",
    "        batch_norm_full_data_sample = self.norm_full_data[idx]\n",
    "        labels_onehot = self.labels_onehot[idx]\n",
    "        adj_matrix_sample = self.adj_matrix[idx]\n",
    "        return idx, batch_norm_missing_data_sample, batch_norm_full_data_sample, labels_onehot, adj_matrix_sample\n",
    "\n",
    "class Dataset_zz:\n",
    "    def __init__(self, opt):\n",
    "        self.data_name = opt.data_name\n",
    "        self.miss_rate = int(100 * opt.miss_rate)\n",
    "        self.id_num = opt.id_num\n",
    "        self.TOPK = opt.TOPK\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.load_data()\n",
    "        self.normalize_data()\n",
    "        self.encode_labels()\n",
    "        self.generate_masks()\n",
    "        self.generate_adjacency_matrices()\n",
    "        self.define_indices() \n",
    "        self.to_tensor() \n",
    "\n",
    "    def load_data(self):\n",
    "        base_path = f'datasets/{self.data_name}/{self.data_name}'\n",
    "        self.missing_data_train = np.array(pd.read_csv(f'{base_path}_train_RANDOM_{self.miss_rate}%_NUM_{self.id_num}.csv', header=None))[:, :-1]\n",
    "        self.missing_data_val = np.array(pd.read_csv(f'{base_path}_val_RANDOM_{self.miss_rate}%_NUM_{self.id_num}.csv', header=None))[:, :-1]\n",
    "        self.missing_data_test = np.array(pd.read_csv(f'{base_path}_test_RANDOM_{self.miss_rate}%_NUM_{self.id_num}.csv', header=None))[:, :-1]\n",
    "        self.missing_data_all = np.concatenate((self.missing_data_train, self.missing_data_val, self.missing_data_test), axis=0)\n",
    "        \n",
    "        self.train_set = np.array(pd.read_csv(f'{base_path}_train_REAL_{self.miss_rate}%_NUM_{self.id_num}.csv', header=None))\n",
    "        self.val_set = np.array(pd.read_csv(f'{base_path}_val_REAL_{self.miss_rate}%_NUM_{self.id_num}.csv', header=None))\n",
    "        self.test_set = np.array(pd.read_csv(f'{base_path}_test_REAL_{self.miss_rate}%_NUM_{self.id_num}.csv', header=None))\n",
    "        self.all_set = np.concatenate((self.train_set, self.val_set, self.test_set), axis=0)\n",
    "\n",
    "        self.train_data, self.train_label = self.train_set[:, :-1], self.train_set[:, -1]\n",
    "        self.val_data, self.val_label = self.val_set[:, :-1], self.val_set[:, -1]\n",
    "        self.test_data, self.test_label = self.test_set[:, :-1], self.test_set[:, -1]\n",
    "        self.all_data, self.all_label = self.all_set[:, :-1], self.all_set[:, -1]\n",
    "\n",
    "    def normalize_data(self):\n",
    "        _, self.all_mean, self.all_std = self.normalization(self.missing_data_all.copy())\n",
    "        self.missing_data_train_norm = self.normalization_with_mean_std(self.missing_data_train.copy(), self.all_mean, self.all_std)\n",
    "        self.missing_data_val_norm = self.normalization_with_mean_std(self.missing_data_val.copy(), self.all_mean, self.all_std)\n",
    "        self.missing_data_test_norm = self.normalization_with_mean_std(self.missing_data_test.copy(), self.all_mean, self.all_std)\n",
    "        self.missing_data_all_norm = self.normalization_with_mean_std(self.missing_data_all.copy(), self.all_mean, self.all_std)\n",
    "        self.train_data_norm = self.normalization_with_mean_std(self.train_data.copy(), self.all_mean, self.all_std)\n",
    "        self.val_data_norm = self.normalization_with_mean_std(self.val_data.copy(), self.all_mean, self.all_std)\n",
    "        self.test_data_norm = self.normalization_with_mean_std(self.test_data.copy(), self.all_mean, self.all_std)\n",
    "        self.all_data_norm = self.normalization_with_mean_std(self.all_data.copy(), self.all_mean, self.all_std)\n",
    "        \n",
    "    @staticmethod\n",
    "    def normalization(data): \n",
    "        temp = np.array(data)\n",
    "        mean = np.nanmean(temp, axis=0) \n",
    "        std = np.nanstd(temp, axis=0) \n",
    "        temp_masked = np.ma.masked_invalid(temp)\n",
    "        temp = (temp_masked - mean) / (std + 1e-8) \n",
    "        temp = temp.filled(np.nan)\n",
    "        return temp, mean, std\n",
    "\n",
    "    @staticmethod\n",
    "    def normalization_with_mean_std(data, mean, std): \n",
    "        temp = np.array(data)\n",
    "        temp_masked = np.ma.masked_invalid(temp) \n",
    "        temp = (temp_masked - mean).astype(np.float32) / (std + 1e-8).astype(np.float32) \n",
    "        temp = temp.filled(np.nan) \n",
    "        return temp\n",
    "\n",
    "    @staticmethod\n",
    "    def denormalization_with_mean_std(data, mean, std):\n",
    "        return data * (std + 1e-8) + mean\n",
    "    \n",
    "    def encode_labels(self):\n",
    "        unique_all_labels = np.unique(np.concatenate((self.train_label, self.val_label, self.test_label)))\n",
    "        self.classes_dict = {label: np.eye(len(unique_all_labels))[i, :] for i, label in enumerate(unique_all_labels)}\n",
    "        self.train_label_onehot = self.encode_onehot(self.train_label)\n",
    "        self.val_label_onehot = self.encode_onehot(self.val_label)\n",
    "        self.test_label_onehot = self.encode_onehot(self.test_label)\n",
    "        self.all_label_onehot = self.encode_onehot(self.all_label)\n",
    "\n",
    "    def encode_onehot(self, labels):\n",
    "        map_func = np.vectorize(self.classes_dict.get, otypes=[np.ndarray])\n",
    "        labels_onehot = np.array(list(map_func(labels)), dtype=np.int32)\n",
    "        return labels_onehot\n",
    "    \n",
    "    def generate_masks(self):\n",
    "        self.mask_train = (~np.isnan(self.missing_data_train)).astype(int)\n",
    "        self.mask_val = (~np.isnan(self.missing_data_val)).astype(int)\n",
    "        self.mask_test = (~np.isnan(self.missing_data_test)).astype(int)\n",
    "        self.mask_all = (~np.isnan(self.missing_data_all)).astype(int)\n",
    "    \n",
    "    def generate_adjacency_matrices(self):\n",
    "        self.adj_train = self.get_adj_euclidean(self.missing_data_train, self.mask_train)\n",
    "        self.adj_val = self.get_adj_euclidean(self.missing_data_val, self.mask_val)\n",
    "        self.adj_test = self.get_adj_euclidean(self.missing_data_test, self.mask_test)\n",
    "        self.adj_all = self.get_adj_euclidean(self.missing_data_all, self.mask_all)\n",
    "        \n",
    "    def get_data_var(self, data, data_mask):\n",
    "        (n_sample, n_attribute) = data.shape\n",
    "        data_var = np.zeros(n_attribute)\n",
    "        for i in range(n_attribute):\n",
    "            valid_data = data[data_mask[:, i] == 1, i]  \n",
    "            if valid_data.size > 0: \n",
    "                data_mean = np.mean(valid_data)  \n",
    "                var = np.sum((valid_data - data_mean) ** 2) / valid_data.size\n",
    "                data_var[i] = var  \n",
    "            else:\n",
    "                data_var[i] = np.nan  \n",
    "        return data_var\n",
    "\n",
    "    def get_dis_euclidean(self, x, y, data_var, x_mask, y_mask):\n",
    "        valid_mask = x_mask * y_mask\n",
    "        valid_indices = np.where(valid_mask == 1)[0]\n",
    "        if valid_indices.size > 0:\n",
    "            x_valid = x[valid_indices]\n",
    "            y_valid = y[valid_indices]\n",
    "            data_var_valid = data_var[valid_indices]\n",
    "            n_attributes = x.shape[0]  \n",
    "            distance_squared_sum = (((x_valid - y_valid) ** 2)/ data_var_valid).sum()\n",
    "            scaled_distance_squared = distance_squared_sum * n_attributes / valid_indices.size\n",
    "            dis = np.sqrt(scaled_distance_squared)\n",
    "        else:\n",
    "            dis = np.inf\n",
    "        return dis\n",
    "\n",
    "    def get_adj_euclidean(self, data, matrix_mask):\n",
    "        n_sample = data.shape[0]\n",
    "        matrix_adj = np.zeros((n_sample, n_sample))\n",
    "        data_var = self.get_data_var(data, matrix_mask)\n",
    "        for i in range(n_sample):\n",
    "            for j in range(i + 1, n_sample):\n",
    "                Dij = self.get_dis_euclidean(data[i], data[j], data_var, matrix_mask[i], matrix_mask[j])\n",
    "                if Dij != 0 and Dij != np.inf: \n",
    "                    matrix_adj[i][j] = matrix_adj[j][i] = 1 / Dij\n",
    "        for i in range(n_sample):\n",
    "            row = matrix_adj[i]\n",
    "            if np.count_nonzero(row) > self.TOPK:\n",
    "                threshold = np.partition(row, -self.TOPK)[-self.TOPK]\n",
    "                matrix_adj[i][matrix_adj[i] < threshold] = 0\n",
    "        row_sums = matrix_adj.sum(axis=1) + 1e-6\n",
    "        matrix_adj = matrix_adj / row_sums[:, np.newaxis]\n",
    "        return matrix_adj\n",
    "    \n",
    "    def define_indices(self):\n",
    "        train_len = len(self.train_data)\n",
    "        val_len = len(self.val_data)\n",
    "        test_len = len(self.test_data)\n",
    "\n",
    "        self.train_indices = range(0, train_len)\n",
    "        self.val_indices = range(train_len, train_len + val_len)\n",
    "        self.test_indices = range(train_len + val_len, train_len + val_len + test_len)\n",
    "\n",
    "        self.train_indices = list(self.train_indices)\n",
    "        self.val_indices = list(self.val_indices)\n",
    "        self.test_indices = list(self.test_indices)\n",
    "        \n",
    "    def to_tensor(self):\n",
    "        self.missing_data_train = torch.from_numpy(self.missing_data_train).float().to(self.device)\n",
    "        self.missing_data_val = torch.from_numpy(self.missing_data_val).float().to(self.device)\n",
    "        self.missing_data_test = torch.from_numpy(self.missing_data_test).float().to(self.device)\n",
    "        self.missing_data_all = torch.from_numpy(self.missing_data_all).float().to(self.device)\n",
    "\n",
    "        self.missing_data_train_norm = torch.from_numpy(self.missing_data_train_norm).float().to(self.device)\n",
    "        self.missing_data_val_norm = torch.from_numpy(self.missing_data_val_norm).float().to(self.device)\n",
    "        self.missing_data_test_norm = torch.from_numpy(self.missing_data_test_norm).float().to(self.device)\n",
    "        self.missing_data_all_norm = torch.from_numpy(self.missing_data_all_norm).float().to(self.device)\n",
    "\n",
    "        self.train_data =  torch.from_numpy(self.train_data).float().to(self.device)\n",
    "        self.val_data =  torch.from_numpy(self.val_data).float().to(self.device)\n",
    "        self.test_data =  torch.from_numpy(self.test_data).float().to(self.device)\n",
    "        self.all_data = torch.from_numpy(self.all_data).float().to(self.device)\n",
    "\n",
    "        self.train_data_norm =  torch.from_numpy(self.train_data_norm).float().to(self.device)\n",
    "        self.val_data_norm =  torch.from_numpy(self.val_data_norm).float().to(self.device)\n",
    "        self.test_data_norm =  torch.from_numpy(self.test_data_norm).float().to(self.device)\n",
    "        self.all_data_norm = torch.from_numpy(self.all_data_norm).float().to(self.device)\n",
    "\n",
    "        self.train_label = torch.from_numpy(self.train_label).float().to(self.device)\n",
    "        self.val_label = torch.from_numpy(self.val_label).float().to(self.device)\n",
    "        self.test_label = torch.from_numpy(self.test_label).float().to(self.device)\n",
    "        self.all_label = torch.from_numpy(self.all_label).float().to(self.device)\n",
    "\n",
    "        self.train_label_onehot = torch.from_numpy(self.train_label_onehot).float().to(self.device)\n",
    "        self.val_label_onehot = torch.from_numpy(self.val_label_onehot).float().to(self.device)\n",
    "        self.test_label_onehot = torch.from_numpy(self.test_label_onehot).float().to(self.device)\n",
    "        self.all_label_onehot = torch.from_numpy(self.all_label_onehot).float().to(self.device)\n",
    "\n",
    "        self.mask_train = torch.from_numpy(self.mask_train).float().to(self.device)\n",
    "        self.mask_val = torch.from_numpy(self.mask_val).float().to(self.device)\n",
    "        self.mask_test = torch.from_numpy(self.mask_test).float().to(self.device)\n",
    "        self.mask_all = torch.from_numpy(self.mask_all).float().to(self.device)\n",
    "        \n",
    "        self.adj_train = torch.from_numpy(self.adj_train).float().to(self.device)\n",
    "        self.adj_val = torch.from_numpy(self.adj_val).float().to(self.device)\n",
    "        self.adj_test = torch.from_numpy(self.adj_test).float().to(self.device)\n",
    "        self.adj_all = torch.from_numpy(self.adj_all).float().to(self.device)\n",
    "\n",
    "        self.all_mean = torch.from_numpy(self.all_mean).float().to(self.device)\n",
    "        self.all_std = torch.from_numpy(self.all_std).float().to(self.device)\n",
    "\n",
    "        self.train_indices = torch.tensor(self.train_indices)\n",
    "        self.val_indices = torch.tensor(self.val_indices)\n",
    "        self.test_indices = torch.tensor(self.test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3b9c2b-08ad-4330-9251-9637a071ab72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "opt.model = 'GTRAE_MVPT_MTL'\n",
    "for opt.data_name in ['banknote']: \n",
    "    for opt.id_num in [2,3,4,5]*3: # 1,2,3,4,5\n",
    "        for opt.miss_rate in [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7]: #0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7\n",
    "            DATA_ZZ = Dataset_zz(opt)\n",
    "            all_data_zz = Dataload_zz(DATA_ZZ.missing_data_all_norm, DATA_ZZ.all_data_norm, DATA_ZZ.all_label_onehot, DATA_ZZ.adj_all) \n",
    "            global_train_idx, global_val_idx, global_test_idx = DATA_ZZ.train_indices, DATA_ZZ.val_indices, DATA_ZZ.test_indices\n",
    "\n",
    "            n_class = DATA_ZZ.train_label_onehot.shape[1] \n",
    "            global_mask_shape = DATA_ZZ.all_data.shape \n",
    "            model = GTRAE_MVPT_MTL(opt.n_hidden, n_class, global_mask_shape, DATA_ZZ, 'all').to(device) \n",
    "            \n",
    "            if opt.load_model_path:\n",
    "                model.load(opt.load_model_path)\n",
    "            \n",
    "            all_dataloader = DataLoader(all_data_zz, opt.batch_size, shuffle=True, num_workers=opt.num_workers)\n",
    "            \n",
    "            criterion_classicfic = torch.nn.NLLLoss().to(device)\n",
    "            criterion_imputation = torch.nn.MSELoss().to(device)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=opt.lr, weight_decay=opt.weight_decay)\n",
    "            \n",
    "            # 初始化存储指标的列表\n",
    "            train_losses, val_losses, test_losses = [], [], []\n",
    "            train_maes, train_rmses, train_mapes, train_accs = [], [], [], []\n",
    "            val_maes, val_rmses, val_mapes, val_accs = [], [], [], []\n",
    "            test_maes, test_rmses, test_mapes, test_accs = [], [], [], []\n",
    "            train_mae_accs, val_mae_accs, test_mae_accs = [], [], []\n",
    "            train_de_maes, train_de_rmses, train_de_mapes = [], [], []\n",
    "            val_de_maes, val_de_rmses, val_de_mapes = [], [], []\n",
    "            test_de_maes, test_de_rmses, test_de_mapes = [], [], []\n",
    "\n",
    "            best_val_loss = float('inf')\n",
    "            patience, triggered = opt.patience, 0\n",
    "            for epoch in range(opt.max_epoch):\n",
    "                epoch_input_data_list = []\n",
    "                epoch_imputed_data_list = []\n",
    "                epoch_target_data_list = [] \n",
    "                epoch_real_data_list = [] \n",
    "                epoch_mask_list = [] \n",
    "                epoch_estimated_label_onehot_list = []\n",
    "                epoch_target_label_onehot_list = []\n",
    "                epoch_all_idx_list = []\n",
    "\n",
    "                for batch_data in all_dataloader:\n",
    "                    batch_all_idx, batch_norm_missing_data, batch_norm_full_data, batch_label_onehot, batch_adj_matrix = batch_data\n",
    "                    batch_mask = ~torch.isnan(batch_norm_missing_data).to(device) \n",
    "                    \n",
    "                    outputs = model(batch_norm_missing_data, batch_all_idx) \n",
    "                    input_data, imputed_data, estimated_label_onehot = outputs \n",
    "                    target_data = batch_norm_full_data * batch_mask + input_data * (~batch_mask) \n",
    "                    target_label_onehot = batch_label_onehot \n",
    "                    target_labels = torch.argmax(target_label_onehot, dim=1) \n",
    "\n",
    "                    mask_train_idx = torch.isin(batch_all_idx, global_train_idx)\n",
    "                    mask_val_idx = torch.isin(batch_all_idx, global_val_idx)\n",
    "                    mask_test_idx = torch.isin(batch_all_idx, global_test_idx)\n",
    "                    batch_train_idx = batch_all_idx[mask_train_idx]\n",
    "                    batch_val_idx = batch_all_idx[mask_val_idx]\n",
    "                    batch_test_idx = batch_all_idx[mask_test_idx]\n",
    "                    batch_train_relative_idx = torch.where(mask_train_idx)\n",
    "                    batch_val_relative_idx = torch.where(mask_val_idx)\n",
    "                    batch_test_relative_idx = torch.where(mask_test_idx)\n",
    "\n",
    "                    loss_imputation = criterion_imputation(imputed_data * batch_mask, target_data * batch_mask) \n",
    "                    loss_imputation = loss_imputation/(1-opt.miss_rate)\n",
    "                    loss_classicfic_train = criterion_classicfic(estimated_label_onehot[batch_train_relative_idx], target_labels[batch_train_relative_idx]) \n",
    "                    batch_model_loss_train = loss_classicfic_train + loss_imputation \n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    batch_model_loss_train.backward(retain_graph=True)\n",
    "                    optimizer.step()\n",
    "\n",
    "                    epoch_input_data_list.append(input_data.detach())\n",
    "                    epoch_imputed_data_list.append(imputed_data.detach())\n",
    "                    epoch_real_data_list.append(batch_norm_full_data.detach()) \n",
    "                    epoch_mask_list.append(batch_mask.detach()) \n",
    "                    epoch_target_data_list.append(target_data.detach())\n",
    "                    epoch_estimated_label_onehot_list.append(estimated_label_onehot.detach())\n",
    "                    epoch_target_label_onehot_list.append(target_label_onehot.detach())\n",
    "                    epoch_all_idx_list.append(batch_all_idx.detach())\n",
    "                ###################################################\n",
    "                #               val and test                      #\n",
    "                ###################################################\n",
    "                epoch_input_data = torch.cat(epoch_input_data_list)\n",
    "                epoch_imputed_data = torch.cat(epoch_imputed_data_list)\n",
    "                epoch_target_data = torch.cat(epoch_target_data_list) \n",
    "                epoch_real_data = torch.cat(epoch_real_data_list) \n",
    "                epoch_mask = torch.cat(epoch_mask_list) \n",
    "                epoch_de_input_data = DATA_ZZ.denormalization_with_mean_std(epoch_input_data.clone(), DATA_ZZ.all_mean, DATA_ZZ.all_std)\n",
    "                epoch_de_imputed_data = DATA_ZZ.denormalization_with_mean_std(epoch_imputed_data.clone(), DATA_ZZ.all_mean, DATA_ZZ.all_std)\n",
    "                epoch_de_target_data = DATA_ZZ.denormalization_with_mean_std(epoch_target_data.clone(), DATA_ZZ.all_mean, DATA_ZZ.all_std) \n",
    "                epoch_de_real_data = DATA_ZZ.denormalization_with_mean_std(epoch_real_data.clone(), DATA_ZZ.all_mean, DATA_ZZ.all_std) \n",
    "                epoch_estimated_label_onehot = torch.cat(epoch_estimated_label_onehot_list)\n",
    "                epoch_target_label_onehot = torch.cat(epoch_target_label_onehot_list)\n",
    "                epoch_all_idx = torch.cat(epoch_all_idx_list)\n",
    "                epoch_estimated_labels = torch.argmax(epoch_estimated_label_onehot, dim=1) \n",
    "                epoch_target_labels = torch.argmax(epoch_target_label_onehot, dim=1) \n",
    "\n",
    "                epoch_mask_train_idx = torch.isin(epoch_all_idx, global_train_idx)\n",
    "                epoch_mask_val_idx = torch.isin(epoch_all_idx, global_val_idx)\n",
    "                epoch_mask_test_idx = torch.isin(epoch_all_idx, global_test_idx)\n",
    "                epoch_batch_train_idx = epoch_all_idx[epoch_mask_train_idx]\n",
    "                epoch_batch_val_idx = epoch_all_idx[epoch_mask_val_idx]\n",
    "                epoch_batch_test_idx = epoch_all_idx[epoch_mask_test_idx]\n",
    "                epoch_train_relative_idx = torch.where(epoch_mask_train_idx)\n",
    "                epoch_val_relative_idx = torch.where(epoch_mask_val_idx)\n",
    "                epoch_test_relative_idx = torch.where(epoch_mask_test_idx)\n",
    "\n",
    "                epoch_loss_imputation = criterion_imputation(epoch_input_data * epoch_mask, epoch_imputed_data * epoch_mask) \n",
    "                epoch_loss_imputation = epoch_loss_imputation/(1-opt.miss_rate)\n",
    "                epoch_loss_classicfic_train = criterion_classicfic(epoch_estimated_label_onehot[epoch_train_relative_idx], epoch_target_labels[epoch_train_relative_idx]) \n",
    "                epoch_loss_classicfic_val = criterion_classicfic(epoch_estimated_label_onehot[epoch_val_relative_idx], epoch_target_labels[epoch_val_relative_idx]) \n",
    "                epoch_loss_classicfic_test = criterion_classicfic(epoch_estimated_label_onehot[epoch_test_relative_idx], epoch_target_labels[epoch_test_relative_idx]) \n",
    "                epoch_model_loss_train = epoch_loss_classicfic_train + epoch_loss_imputation \n",
    "                epoch_model_loss_val = epoch_loss_classicfic_val + epoch_loss_imputation \n",
    "                epoch_model_loss_test = epoch_loss_classicfic_test + epoch_loss_imputation \n",
    "                \n",
    "                epoch_mae_train, epoch_rmse_train, epoch_mape_train = calculate_imputation_metrics(epoch_imputed_data[epoch_train_relative_idx], epoch_target_data[epoch_train_relative_idx])\n",
    "                epoch_mae_val, epoch_rmse_val, epoch_mape_val = calculate_imputation_metrics(epoch_imputed_data[epoch_val_relative_idx], epoch_target_data[epoch_val_relative_idx])\n",
    "                epoch_mae_test, epoch_rmse_test, epoch_mape_test = calculate_imputation_metrics_mask(epoch_imputed_data[epoch_test_relative_idx], epoch_real_data[epoch_test_relative_idx], epoch_mask[epoch_test_relative_idx])\n",
    "                \n",
    "                epoch_de_mae_train, epoch_de_rmse_train, epoch_de_mape_train = calculate_imputation_metrics(epoch_de_imputed_data[epoch_train_relative_idx], epoch_de_target_data[epoch_train_relative_idx])\n",
    "                epoch_de_mae_val, epoch_de_rmse_val, epoch_de_mape_val = calculate_imputation_metrics(epoch_de_imputed_data[epoch_val_relative_idx], epoch_de_target_data[epoch_val_relative_idx])\n",
    "                epoch_de_mae_test, epoch_de_rmse_test, epoch_de_mape_test = calculate_imputation_metrics_mask(epoch_de_imputed_data[epoch_test_relative_idx], epoch_de_real_data[epoch_test_relative_idx], epoch_mask[epoch_test_relative_idx])\n",
    "                \n",
    "                epoch_acc_train = calculate_accuracy(epoch_estimated_labels[epoch_train_relative_idx], epoch_target_labels[epoch_train_relative_idx])\n",
    "                epoch_acc_val = calculate_accuracy(epoch_estimated_labels[epoch_val_relative_idx], epoch_target_labels[epoch_val_relative_idx])\n",
    "                epoch_acc_test = calculate_accuracy(epoch_estimated_labels[epoch_test_relative_idx], epoch_target_labels[epoch_test_relative_idx])\n",
    "\n",
    "                train_losses.append(epoch_model_loss_train.item())\n",
    "                val_losses.append(epoch_model_loss_val.item())\n",
    "                test_losses.append(epoch_model_loss_test.item())\n",
    "                train_maes.append(epoch_mae_train)\n",
    "                train_rmses.append(epoch_rmse_train)\n",
    "                train_mapes.append(epoch_mape_train)\n",
    "                train_accs.append(epoch_acc_train)\n",
    "                val_maes.append(epoch_mae_val)\n",
    "                val_rmses.append(epoch_rmse_val)\n",
    "                val_mapes.append(epoch_mape_val)\n",
    "                val_accs.append(epoch_acc_val)\n",
    "                test_maes.append(epoch_mae_test)\n",
    "                test_rmses.append(epoch_rmse_test)\n",
    "                test_mapes.append(epoch_mape_test)\n",
    "                test_accs.append(epoch_acc_test)\n",
    "                train_de_maes.append(epoch_de_mae_train)\n",
    "                train_de_rmses.append(epoch_de_rmse_train)\n",
    "                train_de_mapes.append(epoch_de_mape_train)\n",
    "                val_de_maes.append(epoch_de_mae_val)\n",
    "                val_de_rmses.append(epoch_de_rmse_val)\n",
    "                val_de_mapes.append(epoch_de_mape_val)\n",
    "                test_de_maes.append(epoch_de_mae_test)\n",
    "                test_de_rmses.append(epoch_de_rmse_test)\n",
    "                test_de_mapes.append(epoch_de_mape_test)\n",
    "\n",
    "                train_mae_acc = epoch_mae_train / epoch_acc_train\n",
    "                val_mae_acc = epoch_mae_val / epoch_acc_val\n",
    "                test_mae_acc = epoch_mae_test / epoch_acc_test\n",
    "\n",
    "                train_mae_accs.append(train_mae_acc)\n",
    "                val_mae_accs.append(val_mae_acc)\n",
    "                test_mae_accs.append(test_mae_acc)\n",
    "\n",
    "                if epoch_model_loss_val < best_val_loss:\n",
    "                    best_val_loss = epoch_model_loss_val\n",
    "                    triggered = 0\n",
    "                else:\n",
    "                    triggered += 1\n",
    "                    if triggered >= patience:\n",
    "                        print(\"Early stopping triggered at epoch {}\".format(epoch))\n",
    "                        break\n",
    "            \n",
    "            min_val_loss_index = val_losses.index(min(val_losses))\n",
    "\n",
    "            data_record = {\n",
    "                'Dataset': opt.data_name,\n",
    "                'Missing Rate':opt.miss_rate,\n",
    "                'Data ID':opt.id_num,\n",
    "                'Model':opt.model,\n",
    "                'Epoch': min_val_loss_index + 1,\n",
    "\n",
    "                'Train Loss': train_losses[min_val_loss_index],\n",
    "                'Validation Loss': val_losses[min_val_loss_index],\n",
    "                'Test Loss': test_losses[min_val_loss_index],\n",
    "\n",
    "                'Train Accuracy': train_accs[min_val_loss_index]*100,\n",
    "                'Val Accuracy': val_accs[min_val_loss_index]*100,\n",
    "                'Test Accuracy': test_accs[min_val_loss_index]*100,\n",
    "\n",
    "                'Train MAE(reconstruct)': train_maes[min_val_loss_index],\n",
    "                'Val MAE(reconstruct)': val_maes[min_val_loss_index],\n",
    "                'Test MAE': test_maes[min_val_loss_index],\n",
    "\n",
    "                'Train RMSE(reconstruct)': train_rmses[min_val_loss_index],\n",
    "                'Val RMSE(reconstruct)': val_rmses[min_val_loss_index],\n",
    "                'Test RMSE': test_rmses[min_val_loss_index],\n",
    "\n",
    "                'Train MAPE(reconstruct)': train_mapes[min_val_loss_index],\n",
    "                'Val MAPE(reconstruct)': val_mapes[min_val_loss_index],\n",
    "                'Test MAPE': test_mapes[min_val_loss_index],\n",
    "\n",
    "                'Train de_MAE(reconstruct)': train_de_maes[min_val_loss_index],\n",
    "                'Val de_MAE(reconstruct)': val_de_maes[min_val_loss_index],\n",
    "                'Test de_MAE': test_de_maes[min_val_loss_index],\n",
    "\n",
    "                'Train de_RMSE(reconstruct)': train_de_rmses[min_val_loss_index],\n",
    "                'Val de_RMSE(reconstruct)': val_de_rmses[min_val_loss_index],\n",
    "                'Test de_RMSE': test_de_rmses[min_val_loss_index],\n",
    "\n",
    "                'Train de_MAPE(reconstruct)': train_de_mapes[min_val_loss_index],\n",
    "                'Val de_MAPE(reconstruct)': val_de_mapes[min_val_loss_index],\n",
    "                'Test de_MAPE': test_de_mapes[min_val_loss_index],\n",
    "                # 'Train MAE*ACC': train_mae_accs[min_val_loss_index],\n",
    "                # 'Val MAE*ACC': val_mae_accs[min_val_loss_index],\n",
    "                # 'Test MAE*ACC': test_mae_accs[min_val_loss_index]\n",
    "            }\n",
    "\n",
    "            filename = f'zz_result/results.csv'\n",
    "            if not os.path.isfile(filename):\n",
    "                df = pd.DataFrame([data_record])\n",
    "            else:\n",
    "                df = pd.read_csv(filename)\n",
    "                df = df.append(data_record, ignore_index=True)\n",
    "\n",
    "            df.to_csv(filename, index=False)\n",
    "            filename = f'zz_result/{opt.data_name}/{opt.data_name}.csv'\n",
    "\n",
    "            if not os.path.isfile(filename):\n",
    "                df = pd.DataFrame([data_record])\n",
    "            else:\n",
    "                df = pd.read_csv(filename)\n",
    "                df = df.append(data_record, ignore_index=True)\n",
    "\n",
    "            df.to_csv(filename, index=False)\n",
    "\n",
    "            plt.figure(figsize=(15, 9))\n",
    "            plt.subplot(3, 3, 1)\n",
    "            plt.plot(train_losses, label='Train Loss')\n",
    "            plt.plot(val_losses, label='Validation Loss')\n",
    "            plt.plot(test_losses, label='Test Loss')\n",
    "            plt.title('Loss over epochs')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "\n",
    "            plt.subplot(3, 3, 2)\n",
    "            plt.plot(train_maes, label='Train MAE')\n",
    "            plt.plot(val_maes, label='Val MAE')\n",
    "            plt.plot(test_maes, label='Test MAE')\n",
    "            plt.title('Metrics over epochs')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.legend()\n",
    "\n",
    "            plt.subplot(3, 3, 3)\n",
    "            plt.plot(train_de_maes, label='Train de_MAE')\n",
    "            plt.plot(val_de_maes, label='Val de_MAE')\n",
    "            plt.plot(test_de_maes, label='Test de_MAE')\n",
    "            plt.title('Metrics over epochs')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.legend()\n",
    "\n",
    "            plt.subplot(3, 3, 4)\n",
    "            plt.plot(train_rmses, label='Train RMSE')\n",
    "            plt.plot(val_rmses, label='Val RMSE')\n",
    "            plt.plot(test_rmses, label='Test RMSE')\n",
    "            plt.title('Metrics over epochs')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.legend()\n",
    "\n",
    "            plt.subplot(3, 3, 5)\n",
    "            plt.plot(train_de_rmses, label='Train de_RMSE')\n",
    "            plt.plot(val_de_rmses, label='Val de_RMSE')\n",
    "            plt.plot(test_de_rmses, label='Test de_RMSE')\n",
    "            plt.title('Metrics over epochs')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.legend()\n",
    "\n",
    "            plt.subplot(3, 3, 6)\n",
    "            plt.plot(train_mapes, label='Train MAPE')\n",
    "            plt.plot(val_mapes, label='Val MAPE')\n",
    "            plt.plot(test_mapes, label='Test MAPE')\n",
    "            plt.title('Metrics over epochs')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.legend()\n",
    "\n",
    "            plt.subplot(3, 3, 7)\n",
    "            plt.plot(train_de_mapes, label='Train de_MAPE')\n",
    "            plt.plot(val_de_mapes, label='Val de_MAPE')\n",
    "            plt.plot(test_de_mapes, label='Test de_MAPE')\n",
    "            plt.title('Metrics over epochs')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.legend()\n",
    "\n",
    "            plt.subplot(3, 3, 8)\n",
    "            plt.plot(train_accs, label='Train Accuracy')\n",
    "            plt.plot(val_accs, label='Val Accuracy')\n",
    "            plt.plot(test_accs, label='Test Accuracy')\n",
    "            plt.title('Metrics over epochs')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.legend()\n",
    "\n",
    "            plt.subplot(3, 3, 9) \n",
    "            plt.plot(train_mae_accs, label='Train MAE/ACC')\n",
    "            plt.plot(val_mae_accs, label='Validation MAE/ACC')\n",
    "            plt.plot(test_mae_accs, label='Test MAE/ACC')\n",
    "            plt.title('MAE*ACC over epochs')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('MAE*ACC')\n",
    "            plt.legend()\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383e989e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
