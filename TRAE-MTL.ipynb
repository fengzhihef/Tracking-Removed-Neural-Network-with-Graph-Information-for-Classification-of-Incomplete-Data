{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9e5048-4bba-43d9-b132-6541c1625350",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 一些参数的设置：学习率、批大小、dropout参数、数据集名称等、训练测试验证比率\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "class DefaultConfig(object):\n",
    "    model = 'TRAE_MVPT_MTL'\n",
    "    use_gpu = True\n",
    "    if use_gpu:\n",
    "        device = 'gpu'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "    load_model_path = None\n",
    "    # 以上模型基本信息\n",
    "    data_name = 'iris'\n",
    "    batch_size = 128\n",
    "    num_workers = 0\n",
    "    max_epoch = 500\n",
    "    patience = patience_acc = patience_mae = 100  # 早停等待轮数\n",
    "    lr = 0.001\n",
    "    lr_decay = 0.97\n",
    "    weight_decay = 1e-5\n",
    "    train_rate = 0.6\n",
    "    val_rate = 0.1\n",
    "    droput = 0.1\n",
    "    miss_rate = 0.05\n",
    "    whiten_rate = 0.1 # 白化率\n",
    "    id_num = 1 # 缺失化的编号（包括划分数据集)\n",
    "    TOPK = 5 #筛选边的最大数量\n",
    "    n_hidden = 30 # 隐藏层神经元个数\n",
    "    use_all_to_train = True # 是否全数据集去训练，用于将缺失值视为变量\n",
    "    model_save_path_acc = 'zz_saved_model/best_model_acc.pth' #保存最优acc模型状态的位置，暂存用于早停\n",
    "    model_save_path_mae = 'zz_saved_model/best_model_mae.pth' #保存最优mae模型状态的位置，暂存用于早停\n",
    "opt = DefaultConfig()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device=\", device)\n",
    "print(\"opt\",opt.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74924929-416a-4fd2-a5ad-c9d30fd22123",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 常用的函数\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "import copy\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import xlwt, xlrd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils import data as torch_data\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score\n",
    "\n",
    "def calculate_imputation_metrics(imputed_data, target_data):\n",
    "    mae = mean_absolute_error(target_data.cpu().numpy(), imputed_data.cpu().numpy())\n",
    "    rmse = np.sqrt(mean_squared_error(target_data.cpu().numpy(), imputed_data.cpu().numpy()))\n",
    "    mape = np.mean(np.abs((target_data.cpu().numpy() - imputed_data.cpu().numpy()) / (target_data.cpu().numpy()+1e-8))) * 100\n",
    "    \n",
    "    return mae, rmse, mape\n",
    "\n",
    "# 计算填补损失的MAE, RMSE, MAPE\n",
    "def calculate_imputation_metrics_mask(imputed_data, target_data, mask): # 有效值mask就行\n",
    "    # 确保数据和mask都转移到CPU并转换为NumPy数组\n",
    "    imputed_data_np = imputed_data.cpu().numpy()\n",
    "    target_data_np = target_data.cpu().numpy()\n",
    "    mask_np = mask.cpu().numpy()\n",
    "    \n",
    "    # 使用mask=0过滤数据\n",
    "    filtered_imputed_data = imputed_data_np[mask_np == 0]\n",
    "    filtered_target_data = target_data_np[mask_np == 0]\n",
    "\n",
    "    # 计算MAE\n",
    "    mae = mean_absolute_error(filtered_target_data, filtered_imputed_data)\n",
    "    \n",
    "    # 计算RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(filtered_target_data, filtered_imputed_data))\n",
    "    \n",
    "    # 计算MAPE，向分母添加一个小的正数以确保不会因为除以零而出错\n",
    "    epsilon = 1e-8\n",
    "    mape = np.mean(np.abs((filtered_target_data - filtered_imputed_data) / (filtered_target_data + epsilon))) * 100\n",
    "    \n",
    "    return mae, rmse, mape\n",
    "\n",
    "# 计算分类准确率\n",
    "def calculate_accuracy(estimated_label, target_labels):\n",
    "    # estimated_label = torch.argmax(estimated_label, dim=1)\n",
    "    acc = accuracy_score(target_labels.cpu().numpy(), estimated_label.cpu().numpy())\n",
    "    \n",
    "    return acc\n",
    "\n",
    "# 写入数据行\n",
    "def writeline_csv(filename, rmse, mae, mape, acc, norm_rmse, norm_mae, norm_mape):\n",
    "    file_exists = os.path.isfile(filename)\n",
    "    with open(filename, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # 检查文件是否存在，如果不存在，写入头部\n",
    "        if not file_exists:\n",
    "            writer.writerow(['Dataset','Miss Rate', 'RMSE', 'MAE', 'MAPE', 'Acc', 'Norm RMSE', 'Norm MAE', 'Norm MAPE'])\n",
    "        # 写入数据行\n",
    "        writer.writerow([opt.data_name, opt.miss_rate, rmse, mae, mape, acc, norm_rmse, norm_mae, norm_mape])\n",
    "\n",
    "def whiten(tensor_,whiten_rate = 0.05):\n",
    "    tensor = tensor_.clone()\n",
    "    attr = tensor.shape[1] // 3  # 定义attr的大小\n",
    "    mask = tensor[:, :attr]  # 提取mask部分\n",
    "\n",
    "    # 找到mask中所有的有效值（1）的索引\n",
    "    valid_indices = torch.where(mask == 1) # 这里按照阶给的，所以(valid_indices[0][i],valid_indices[1][i])就是第i个有效值的位置\n",
    "    # 计算有效值5%的数量\n",
    "    num_to_whiten = int(whiten_rate * len(valid_indices[0]))\n",
    "    # 如果需要变为0的数量大于0，则执行变更\n",
    "    if num_to_whiten > 0:\n",
    "        # 随机选择一部分有效值的索引来变为0\n",
    "        indices_to_whiten = torch.randperm(len(valid_indices[0]))[:num_to_whiten]\n",
    "        # 将这些随机选择的有效值设置为0\n",
    "        mask[valid_indices[0][indices_to_whiten], valid_indices[1][indices_to_whiten]] = 0\n",
    "\n",
    "    # 更新原tensor的mask部分\n",
    "    tensor[:, :attr] = mask\n",
    "    return tensor\n",
    "\n",
    "# 定义一个自定义的MSE损失函数，仅计算mask==1的位置的误差\n",
    "def masked_mse_loss(input_, target, mask):\n",
    "    # 应用mask，仅保留有效（非缺失）部分\n",
    "    masked_input = torch.mul(input_, mask)\n",
    "    masked_target = torch.mul(target, mask)\n",
    "    \n",
    "    # 计算有效部分的MSE损失\n",
    "    loss = torch.sum((masked_input - masked_target) ** 2) / (torch.sum(mask)+1e-8)\n",
    "    return loss\n",
    "\n",
    "# 评估函数\n",
    "def evaluate(opt, model, val_dataloader, feature_label_mask_miss, adj_euclidean, adj_mahalanobis, real_features, norm_real_features, max_zz, min_zz, data_mask_add):\n",
    "    model.eval()\n",
    "    predict_all, labels_all = [], []\n",
    "    RMSE_total, MAE_total, MAPE_total, num_total = 0., 0., 0., 0.\n",
    "    RMSE_norm, MAE_norm, MAPE_norm = 0., 0., 0.  # 归一化指标初始化\n",
    "    num_valid = 0 # 有效值的个数，对于多个batch的情况，那就需要累加，而不是一次性计算得到\n",
    "    with torch.no_grad():\n",
    "        for batch_evaluate_idx, batch_feature_label_mask_miss in val_dataloader:\n",
    "            # 获得关键的参数\n",
    "            attr_dim = real_features.shape[1] #属性维度\n",
    "            # 根据batch调集子图和子数据输入模型，得到输出\n",
    "            batch_data_mask_add = data_mask_add[batch_evaluate_idx] #真实的附加mask\n",
    "            sub_adj_euclidean = adj_euclidean[batch_evaluate_idx][:, batch_evaluate_idx] #真实的子图\n",
    "            sub_adj_mahalanobis = adj_mahalanobis[batch_evaluate_idx][:, batch_evaluate_idx] #真实的子图\n",
    "            outputs = model(batch_feature_label_mask_miss[:,:attr_dim], sub_adj_euclidean, sub_adj_mahalanobis, batch_data_mask_add, batch_evaluate_idx)\n",
    "            # 获得关键的参数\n",
    "            total_dim = batch_feature_label_mask_miss.shape[1] # 总维度数\n",
    "            # attr_dim = outputs[0].shape[1] #属性维度\n",
    "            label_dim = total_dim - 2 * attr_dim #标签维度\n",
    "            mask = batch_feature_label_mask_miss[:, attr_dim + label_dim:] #输入的掩码矩阵\n",
    "            norm_input_features = batch_feature_label_mask_miss[:, :attr_dim] * mask #输入的不完整特征，缺失位置为99999->0\n",
    "            input_features = de_normalize_zz(norm_input_features, max_zz, min_zz) # 输入的不完整特征，其实用不上，但是可以打印看看作为参考\n",
    "            #！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！\n",
    "            real_features = features_ori[batch_evaluate_idx] #真实的特征 ！！！！！！！！！本质上是不能用于验证集的，只能用于最终估计！！！！！！！！！！！！！\n",
    "            norm_real_features = norm_features_ori[batch_evaluate_idx] #归一化后的真实特征\n",
    "            #############################################################################\n",
    "            norm_imputed_features = outputs[0].clone() #不完整特征填补结果\n",
    "            labels_onehot = feature_label_mask_miss[batch_evaluate_idx][:, attr_dim:attr_dim + label_dim] #真实的标签独热版（真实的）\n",
    "            target_labels = torch.max(labels_onehot, 1)[1] #输入的标签（真实的）（验证集/测试集）\n",
    "            estimed_labels = outputs[1] # 估计的标签（验证集/测试集）\n",
    "            # 特征填补和反归一化\n",
    "            imputed_features = de_normalize_zz(norm_imputed_features, max_zz, min_zz)\n",
    "            # 计算未归一化指标\n",
    "            diff = imputed_features - real_features #实际值差距\n",
    "            mask_indices = np.where(mask.cpu().numpy() == 0) #取缺失位置\n",
    "            diff_masked = diff[mask_indices] #缺失值位置的填补误差\n",
    "            real_features_masked = real_features[mask_indices] #用于mape\n",
    "            \n",
    "            # 计算归一化指标\n",
    "            norm_diff = norm_imputed_features - norm_real_features #归一化的值的差距\n",
    "            norm_diff_masked = norm_diff[mask_indices] #取缺失位置的填补误差\n",
    "            norm_real_features_masked = norm_real_features[mask_indices] #用于mape\n",
    "            \n",
    "            # 计算未归一化指标\n",
    "            RMSE_total += np.sum(np.square(diff_masked.cpu().numpy())) #实际RMSE\n",
    "            MAE_total += np.sum(np.abs(diff_masked.cpu().numpy())) \n",
    "            MAPE_total += np.sum(np.abs(diff_masked.cpu().numpy() / (1e-9+real_features_masked.cpu().numpy()))) * 100\n",
    "            # 计算归一化指标 (直接使用outputs和labels进行计算)\n",
    "            RMSE_norm += np.sum(np.square(norm_diff_masked.cpu().numpy())) #归一化的RMSE\n",
    "            MAE_norm += np.sum(np.abs(norm_diff_masked.cpu().numpy()))\n",
    "            MAPE_norm += np.sum(np.abs(norm_diff_masked.cpu().numpy() / (1e-9+norm_real_features_masked.cpu().numpy()))) * 100\n",
    "            \n",
    "            # 分类结果的准确率计算\n",
    "            predictions = torch.max(outputs[1], 1)[1] #估计的标签\n",
    "            predict_all.extend(predictions.cpu())  # 估计的标签\n",
    "            labels_all.extend(target_labels.cpu()) # 真实的标签\n",
    "            num_valid += np.sum(1-mask.cpu().numpy()) #增加本轮缺失值个数，评估的时候是评估的缺失值的填补效果\n",
    "    # 计算平均指标\n",
    "    rmse = np.sqrt(RMSE_total / (num_valid+1e-8))\n",
    "    mae = MAE_total / (num_valid+1e-8)\n",
    "    mape = MAPE_total / (num_valid+1e-8)\n",
    "    rmse_norm = np.sqrt(RMSE_norm / (num_valid+1e-8))\n",
    "    mae_norm = MAE_norm / (num_valid+1e-8)\n",
    "    mape_norm = MAPE_norm / (num_valid+1e-8)\n",
    "    acc = metrics.accuracy_score(labels_all, predict_all) if labels_all else 0\n",
    "\n",
    "    return rmse, mae, mape, rmse_norm, mae_norm, mape_norm, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286ec801-0072-4962-9038-574cb4b1f3b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 模型架构\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "class BasicModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicModule, self).__init__()\n",
    "        self.model_name = str(type(self))\n",
    "\n",
    "    def load(self, path):\n",
    "        self.load_state_dict(torch.load(path))\n",
    "\n",
    "    def save(self, name=None):\n",
    "        if name is None:\n",
    "            prefix = './checkpoints/' + self.model_name\n",
    "        torch.save(self.state_dict(), prefix)\n",
    "        return name\n",
    "\n",
    "class TRAE_MVPT_MTL(BasicModule):\n",
    "    def __init__(self, hidden_dim, num_classes, global_mask_shape, data_class, model_type):\n",
    "        super(TRAE_MVPT_MTL, self).__init__()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        input_feature_dim = global_mask_shape[1]\n",
    "        self.model_name = 'TRAE_MVPT_MTL' # 模型名\n",
    "        self.dropout_layer = nn.Dropout(p=opt.droput) # 随机无效化层 \n",
    "        self.relu = nn.ReLU() # 激活函数\n",
    "        self.graph_sage_layer1 = Graphsage(input_feature_dim, hidden_dim) # 输入-隐藏\n",
    "        self.graph_sage_layer2 = Graphsage_hidden(hidden_dim, hidden_dim) # 隐藏（+2*输入）-隐藏\n",
    "        self.feature_transform = nn.Linear(hidden_dim, input_feature_dim) # 隐藏转化为特征输出\n",
    "        self.class_transform = nn.Linear(hidden_dim, num_classes) # 隐藏转化为分类输出\n",
    "        # 初始化全局缺失值参数矩阵\n",
    "        self.global_missing_values = nn.Parameter(torch.rand(global_mask_shape))\n",
    "        self.data_class = data_class\n",
    "    def forward(self, norm_missing_data, batch_idx):\n",
    "        # 获取mask\n",
    "        batch_mask = ~torch.isnan(norm_missing_data).to(self.device)\n",
    "        # 使用batch_indices从全局缺失值参数矩阵中提取当前batch的缺失值参数\n",
    "        batch_missing_values = self.global_missing_values[batch_idx]\n",
    "        # 将缺失值替换为缺失值变量的当前值\n",
    "        input_data = torch.where(batch_mask.bool(), norm_missing_data, batch_missing_values)\n",
    "        # 输入层-隐藏层(输入属性去跟踪处理，融入图信息)\n",
    "        hidden_states_1 = self.graph_sage_layer1(input_data)\n",
    "        # # 隐藏层-隐藏层（隐藏状态的中间过渡，不融入图信息）\n",
    "        hidden_states_2 = self.graph_sage_layer2(hidden_states_1)\n",
    "        #用于存放填补结果\n",
    "        concatenated_result = torch.tensor([], device=self.device)\n",
    "        # 线性层-填补输出，最后一个隐藏状态是不去跟踪的，用于分类\n",
    "        for j, hidden in enumerate(hidden_states_2[:-1]):\n",
    "            transformed_data_j = self.feature_transform(hidden)\n",
    "            feature_j = transformed_data_j[:, j].unsqueeze(1)  # 使用unsqueeze保持维度一致性，以便于拼接\n",
    "            concatenated_result = torch.cat((concatenated_result, feature_j), dim=1) # 将这个特征与之前的结果进行拼接\n",
    "        # 线性层-分类输出\n",
    "        class_logits = self.class_transform(hidden_states_2[-1])\n",
    "        # 分类结果经过激活函数，强化锐度，填补因为是经过了Z归一化的，所以不用\n",
    "        imputed_data = concatenated_result\n",
    "        estimated_label = F.log_softmax(class_logits, dim=1)\n",
    "\n",
    "        return input_data, imputed_data, estimated_label\n",
    "\n",
    "class Graphsage(nn.Module):  # 聚合信息的层\n",
    "    def __init__(self, input_feature_dim, output_feature_dim):\n",
    "        super(Graphsage, self).__init__()\n",
    "        self.in_features = input_feature_dim\n",
    "        self.model_name = 'Graphsage'\n",
    "        self.W = nn.Parameter(torch.zeros(size=(1 * input_feature_dim, output_feature_dim)))\n",
    "        self.bias = nn.Parameter(torch.zeros(output_feature_dim))\n",
    "        self.reset_parameters()\n",
    "        self.bn = nn.BatchNorm1d(output_feature_dim)  # 添加BatchNorm层\n",
    "        \n",
    "    def reset_parameters(self):  # 初始化参数\n",
    "        stdv = 1. / (math.sqrt(self.W.size(1)) + 1e-8)\n",
    "        self.W.data.uniform_(-stdv, stdv)\n",
    "        self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        hidden_states = []  # 用于存储每个属性对应的隐藏状态\n",
    "        n_attributes = input_data.size(1)  # 输入属性的数量\n",
    "        \n",
    "        # 拼接特征和聚合信息\n",
    "        concatenated_features = input_data\n",
    "        for j in range(n_attributes): # 去跟踪的实现\n",
    "            # 创建一个新的concatenated_features副本以便修改\n",
    "            modified_features = concatenated_features.clone()\n",
    "            # 将属性j对应的输入concatenated_features中对应的位置的值置为0\n",
    "            modified_features[:, j] = 0\n",
    "            # # 将属性j对应的aggregated_data中对应的位置的值也置为0\n",
    "            # modified_features[:, n_attributes + j] = 0\n",
    "\n",
    "            # 计算transformed_features\n",
    "            transformed_features = torch.mm(modified_features, self.W) + self.bias\n",
    "            # transformed_features = self.bn(transformed_features)  # 应用BatchNorm\n",
    "            \n",
    "            # 将计算得到的transformed_features加入列表\n",
    "            hidden_states.append(transformed_features)\n",
    "\n",
    "        # 用于分类的没有去跟踪的隐藏状态\n",
    "        hidden_states.append(torch.mm(concatenated_features, self.W) + self.bias)\n",
    "        \n",
    "        # 返回所有隐藏状态的列表\n",
    "        return hidden_states\n",
    "\n",
    "class Graphsage_hidden(nn.Module): #聚合隐藏信息的层\n",
    "    def __init__(self, hidden_feature_dim, output_feature_dim):\n",
    "        super(Graphsage_hidden, self).__init__()\n",
    "        self.model_name = 'Graphsage_hidden'\n",
    "        self.W = nn.Parameter(torch.zeros(size=(hidden_feature_dim, output_feature_dim)))\n",
    "        self.bias = nn.Parameter(torch.zeros(output_feature_dim))\n",
    "        self.reset_parameters()\n",
    "        self.bn = nn.BatchNorm1d(output_feature_dim)  # 添加BatchNorm层\n",
    "\n",
    "    def reset_parameters(self): # 初始化参数\n",
    "        # Initialize parameters uniformly based on layer dimensions\n",
    "        stdv = 1. / math.sqrt(self.W.size(1))\n",
    "        self.W.data.uniform_(-stdv, stdv)\n",
    "        self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        new_hidden_states = []\n",
    "        # 计算下一个隐藏状态列表（注意，前s个是去跟踪了的，最后一个是不去跟踪的用于分类的）\n",
    "        for hidden in hidden_states:\n",
    "            transformed_hidden = torch.mm(hidden, self.W) + self.bias #做一个映射，返回原本的属性维度\n",
    "            new_hidden_states.append(transformed_hidden)\n",
    "        return new_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164677f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据类\n",
    "from torch.utils import data as torch_data\n",
    "import numpy as np\n",
    "\n",
    "class Dataload_zz(torch_data.Dataset):\n",
    "    def __init__(self, norm_missing_data, norm_full_data, labels_onehot):\n",
    "        self.norm_missing_data = norm_missing_data #归一化的缺失数据样本\n",
    "        self.norm_full_data = norm_full_data #归一化的完整数据样本\n",
    "        self.labels_onehot = labels_onehot # 标签\n",
    "\n",
    "    def __len__(self):\n",
    "        # 数据集的总长度等于任一数据的长度\n",
    "        return len(self.norm_missing_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_norm_missing_data_sample = self.norm_missing_data[idx]\n",
    "        batch_norm_full_data_sample = self.norm_full_data[idx]\n",
    "        labels_onehot = self.labels_onehot[idx]\n",
    "        return idx, batch_norm_missing_data_sample, batch_norm_full_data_sample, labels_onehot\n",
    "\n",
    "\n",
    "class Dataset_zz:\n",
    "    def __init__(self, opt):\n",
    "        self.data_name = opt.data_name\n",
    "        self.miss_rate = int(100 * opt.miss_rate)\n",
    "        self.id_num = opt.id_num\n",
    "        self.TOPK = opt.TOPK\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.load_data()\n",
    "        self.normalize_data()\n",
    "        self.encode_labels()\n",
    "        self.generate_masks()\n",
    "        # self.generate_adjacency_matrices() # TRAE_MVPT_MTL不使用邻接矩阵\n",
    "        self.define_indices() # 定义索引\n",
    "        self.to_tensor() # 转device上的tensor\n",
    "    def load_data(self):\n",
    "        base_path = f'datasets/{self.data_name}/{self.data_name}'\n",
    "        self.missing_data_train = np.array(pd.read_csv(f'{base_path}_train_RANDOM_{self.miss_rate}%_NUM_{self.id_num}.csv', header=None))[:, :-1]\n",
    "        self.missing_data_val = np.array(pd.read_csv(f'{base_path}_val_RANDOM_{self.miss_rate}%_NUM_{self.id_num}.csv', header=None))[:, :-1]\n",
    "        self.missing_data_test = np.array(pd.read_csv(f'{base_path}_test_RANDOM_{self.miss_rate}%_NUM_{self.id_num}.csv', header=None))[:, :-1]\n",
    "        self.missing_data_all = np.concatenate((self.missing_data_train, self.missing_data_val, self.missing_data_test), axis=0)\n",
    "        \n",
    "        self.train_set = np.array(pd.read_csv(f'{base_path}_train_REAL_{self.miss_rate}%_NUM_{self.id_num}.csv', header=None))\n",
    "        self.val_set = np.array(pd.read_csv(f'{base_path}_val_REAL_{self.miss_rate}%_NUM_{self.id_num}.csv', header=None))\n",
    "        self.test_set = np.array(pd.read_csv(f'{base_path}_test_REAL_{self.miss_rate}%_NUM_{self.id_num}.csv', header=None))\n",
    "        self.all_set = np.concatenate((self.train_set, self.val_set, self.test_set), axis=0)\n",
    "\n",
    "        self.train_data, self.train_label = self.train_set[:, :-1], self.train_set[:, -1]\n",
    "        self.val_data, self.val_label = self.val_set[:, :-1], self.val_set[:, -1]\n",
    "        self.test_data, self.test_label = self.test_set[:, :-1], self.test_set[:, -1]\n",
    "        self.all_data, self.all_label = self.all_set[:, :-1], self.all_set[:, -1]\n",
    "\n",
    "    def normalize_data(self):\n",
    "        _, self.all_mean, self.all_std = self.normalization(self.missing_data_all.copy())\n",
    "        self.missing_data_train_norm = self.normalization_with_mean_std(self.missing_data_train.copy(), self.all_mean, self.all_std)\n",
    "        self.missing_data_val_norm = self.normalization_with_mean_std(self.missing_data_val.copy(), self.all_mean, self.all_std)\n",
    "        self.missing_data_test_norm = self.normalization_with_mean_std(self.missing_data_test.copy(), self.all_mean, self.all_std)\n",
    "        self.missing_data_all_norm = self.normalization_with_mean_std(self.missing_data_all.copy(), self.all_mean, self.all_std)\n",
    "        self.train_data_norm = self.normalization_with_mean_std(self.train_data.copy(), self.all_mean, self.all_std)\n",
    "        self.val_data_norm = self.normalization_with_mean_std(self.val_data.copy(), self.all_mean, self.all_std)\n",
    "        self.test_data_norm = self.normalization_with_mean_std(self.test_data.copy(), self.all_mean, self.all_std)\n",
    "        self.all_data_norm = self.normalization_with_mean_std(self.all_data.copy(), self.all_mean, self.all_std)\n",
    "        \n",
    "    # 数据Z-SCORE标准化\n",
    "    @staticmethod\n",
    "    def normalization(data): \n",
    "        temp = np.array(data)\n",
    "        mean = np.nanmean(temp, axis=0) # 计算均值时忽略nan\n",
    "        std = np.nanstd(temp, axis=0) # 计算标准差时忽略nan\n",
    "        temp_masked = np.ma.masked_invalid(temp) # 创建掩码数组，将nan替换为掩码值\n",
    "        temp = (temp_masked - mean) / (std + 1e-8) # 增加了一个小常数防止除以0\n",
    "        temp = temp.filled(np.nan) # 将掩码值替换为nan\n",
    "        return temp, mean, std\n",
    "\n",
    "    # 使用已知的均值和标准差进行Z-SCORE标准化\n",
    "    @staticmethod\n",
    "    def normalization_with_mean_std(data, mean, std): \n",
    "        temp = np.array(data)\n",
    "        temp_masked = np.ma.masked_invalid(temp) # 创建掩码数组，将nan替换为掩码值\n",
    "        temp = (temp_masked - mean).astype(np.float32) / (std + 1e-8).astype(np.float32) # 增加了一个小常数\n",
    "        temp = temp.filled(np.nan) # 将掩码值替换为nan\n",
    "        return temp\n",
    "\n",
    "    # 反Z-SCORE标准化\n",
    "    @staticmethod\n",
    "    def denormalization_with_mean_std(data, mean, std):\n",
    "        return data * (std + 1e-8) + mean\n",
    "    \n",
    "    def encode_labels(self):\n",
    "        # 步骤 1: 确定所有数据集中所有唯一类别的并集\n",
    "        unique_all_labels = np.unique(np.concatenate((self.train_label, self.val_label, self.test_label)))\n",
    "        # 步骤 2: 基于所有唯一类别创建独热编码映射\n",
    "        self.classes_dict = {label: np.eye(len(unique_all_labels))[i, :] for i, label in enumerate(unique_all_labels)}\n",
    "        # 对所有数据集应用独热编码\n",
    "        self.train_label_onehot = self.encode_onehot(self.train_label)\n",
    "        self.val_label_onehot = self.encode_onehot(self.val_label)\n",
    "        self.test_label_onehot = self.encode_onehot(self.test_label)\n",
    "        self.all_label_onehot = self.encode_onehot(self.all_label)\n",
    "\n",
    "    def encode_onehot(self, labels):\n",
    "        # 使用 np.vectorize 以向量化的方式应用映射，提高效率\n",
    "        map_func = np.vectorize(self.classes_dict.get, otypes=[np.ndarray])\n",
    "        labels_onehot = np.array(list(map_func(labels)), dtype=np.int32)\n",
    "        return labels_onehot\n",
    "    \n",
    "    def generate_masks(self):\n",
    "        # 为训练、验证和测试集的缺失数据生成掩码矩阵\n",
    "        self.mask_train = (~np.isnan(self.missing_data_train)).astype(int)\n",
    "        self.mask_val = (~np.isnan(self.missing_data_val)).astype(int)\n",
    "        self.mask_test = (~np.isnan(self.missing_data_test)).astype(int)\n",
    "        self.mask_all = (~np.isnan(self.missing_data_all)).astype(int)\n",
    "    \n",
    "    def generate_adjacency_matrices(self):\n",
    "        self.adj_train = self.get_adj_euclidean(self.missing_data_train, self.mask_train)\n",
    "        self.adj_val = self.get_adj_euclidean(self.missing_data_val, self.mask_val)\n",
    "        self.adj_test = self.get_adj_euclidean(self.missing_data_test, self.mask_test)\n",
    "        self.adj_all = self.get_adj_euclidean(self.missing_data_all, self.mask_all)\n",
    "        \n",
    "    # 获取数据的每个属性的方差\n",
    "    def get_data_var(self, data, data_mask):\n",
    "        (n_sample, n_attribute) = data.shape\n",
    "        data_var = np.zeros(n_attribute)\n",
    "        # 遍历每个属性\n",
    "        for i in range(n_attribute):\n",
    "            valid_data = data[data_mask[:, i] == 1, i]  # 仅选择该属性中有效的数据\n",
    "            if valid_data.size > 0:  # 确保有有效数据\n",
    "                data_mean = np.mean(valid_data)  # 计算有效数据的均值\n",
    "                # 计算有效数据的方差\n",
    "                var = np.sum((valid_data - data_mean) ** 2) / valid_data.size\n",
    "                data_var[i] = var  # 存储每个属性的方差\n",
    "            else:\n",
    "                data_var[i] = np.nan  # 如果没有有效数据，则标记为NaN\n",
    "        return data_var\n",
    "\n",
    "    # 计算两个样本之间的距离\n",
    "    def get_dis_euclidean(self, x, y, data_var, x_mask, y_mask):\n",
    "        # 获取同时在x和y中有效的位置\n",
    "        valid_mask = x_mask * y_mask\n",
    "        valid_indices = np.where(valid_mask == 1)[0]\n",
    "        # 仅选取有效位置的值进行距离计算\n",
    "        if valid_indices.size > 0:\n",
    "            x_valid = x[valid_indices]\n",
    "            y_valid = y[valid_indices]\n",
    "            data_var_valid = data_var[valid_indices]\n",
    "            n_attributes = x.shape[0]  # 总属性数\n",
    "            # 计算有效位置上的距离平方和, 还需要除以属性的方差使得每个属性的总差异（用方差/标准差衡量的差异，都是1）的重要性等同\n",
    "            distance_squared_sum = (((x_valid - y_valid) ** 2)/ data_var_valid).sum()\n",
    "            # 缩放距离平方和：将距离平方和除以有效值个数，乘以属性总数\n",
    "            scaled_distance_squared = distance_squared_sum * n_attributes / valid_indices.size\n",
    "            # 计算缩放后的欧式距离\n",
    "            dis = np.sqrt(scaled_distance_squared)\n",
    "        else:\n",
    "            # 如果没有有效的共同位置，则距离设为无穷大或其他标记值\n",
    "            dis = np.inf\n",
    "        return dis\n",
    "\n",
    "    # 构建邻接矩阵\n",
    "    def get_adj_euclidean(self, data, matrix_mask):\n",
    "        n_sample = data.shape[0]\n",
    "        matrix_adj = np.zeros((n_sample, n_sample))\n",
    "        data_var = self.get_data_var(data, matrix_mask)\n",
    "        for i in range(n_sample):\n",
    "            for j in range(i + 1, n_sample):\n",
    "                Dij = self.get_dis_euclidean(data[i], data[j], data_var, matrix_mask[i], matrix_mask[j])\n",
    "                if Dij != 0 and Dij != np.inf: \n",
    "                    matrix_adj[i][j] = matrix_adj[j][i] = 1 / Dij\n",
    "        for i in range(n_sample):\n",
    "            row = matrix_adj[i]\n",
    "            if np.count_nonzero(row) > self.TOPK:\n",
    "                threshold = np.partition(row, -self.TOPK)[-self.TOPK]\n",
    "                matrix_adj[i][matrix_adj[i] < threshold] = 0\n",
    "        row_sums = matrix_adj.sum(axis=1) + 1e-6\n",
    "        matrix_adj = matrix_adj / row_sums[:, np.newaxis]\n",
    "        return matrix_adj\n",
    "    \n",
    "    def define_indices(self):\n",
    "        # 计算训练、验证、测试数据的长度\n",
    "        train_len = len(self.train_data)\n",
    "        val_len = len(self.val_data)\n",
    "        test_len = len(self.test_data)\n",
    "\n",
    "        # 根据数据长度计算索引范围\n",
    "        self.train_indices = range(0, train_len)\n",
    "        self.val_indices = range(train_len, train_len + val_len)\n",
    "        self.test_indices = range(train_len + val_len, train_len + val_len + test_len)\n",
    "\n",
    "        # 也可以将索引转换为list，便于后续操作\n",
    "        self.train_indices = list(self.train_indices)\n",
    "        self.val_indices = list(self.val_indices)\n",
    "        self.test_indices = list(self.test_indices)\n",
    "        \n",
    "    def to_tensor(self): #将所有数据转换为tensor，并移动到指定的device上\n",
    "        self.missing_data_train = torch.from_numpy(self.missing_data_train).float().to(self.device)\n",
    "        self.missing_data_val = torch.from_numpy(self.missing_data_val).float().to(self.device)\n",
    "        self.missing_data_test = torch.from_numpy(self.missing_data_test).float().to(self.device)\n",
    "        self.missing_data_all = torch.from_numpy(self.missing_data_all).float().to(self.device)\n",
    "\n",
    "        self.missing_data_train_norm = torch.from_numpy(self.missing_data_train_norm).float().to(self.device)\n",
    "        self.missing_data_val_norm = torch.from_numpy(self.missing_data_val_norm).float().to(self.device)\n",
    "        self.missing_data_test_norm = torch.from_numpy(self.missing_data_test_norm).float().to(self.device)\n",
    "        self.missing_data_all_norm = torch.from_numpy(self.missing_data_all_norm).float().to(self.device)\n",
    "\n",
    "        self.train_data =  torch.from_numpy(self.train_data).float().to(self.device)\n",
    "        self.val_data =  torch.from_numpy(self.val_data).float().to(self.device)\n",
    "        self.test_data =  torch.from_numpy(self.test_data).float().to(self.device)\n",
    "        self.all_data = torch.from_numpy(self.all_data).float().to(self.device)\n",
    "\n",
    "        self.train_data_norm =  torch.from_numpy(self.train_data_norm).float().to(self.device)\n",
    "        self.val_data_norm =  torch.from_numpy(self.val_data_norm).float().to(self.device)\n",
    "        self.test_data_norm =  torch.from_numpy(self.test_data_norm).float().to(self.device)\n",
    "        self.all_data_norm = torch.from_numpy(self.all_data_norm).float().to(self.device)\n",
    "\n",
    "        self.train_label = torch.from_numpy(self.train_label).float().to(self.device)\n",
    "        self.val_label = torch.from_numpy(self.val_label).float().to(self.device)\n",
    "        self.test_label = torch.from_numpy(self.test_label).float().to(self.device)\n",
    "        self.all_label = torch.from_numpy(self.all_label).float().to(self.device)\n",
    "\n",
    "        self.train_label_onehot = torch.from_numpy(self.train_label_onehot).float().to(self.device)\n",
    "        self.val_label_onehot = torch.from_numpy(self.val_label_onehot).float().to(self.device)\n",
    "        self.test_label_onehot = torch.from_numpy(self.test_label_onehot).float().to(self.device)\n",
    "        self.all_label_onehot = torch.from_numpy(self.all_label_onehot).float().to(self.device)\n",
    "\n",
    "        self.mask_train = torch.from_numpy(self.mask_train).float().to(self.device)\n",
    "        self.mask_val = torch.from_numpy(self.mask_val).float().to(self.device)\n",
    "        self.mask_test = torch.from_numpy(self.mask_test).float().to(self.device)\n",
    "        self.mask_all = torch.from_numpy(self.mask_all).float().to(self.device)\n",
    "        \n",
    "        # self.adj_train = torch.from_numpy(self.adj_train).float().to(self.device)\n",
    "        # self.adj_val = torch.from_numpy(self.adj_val).float().to(self.device)\n",
    "        # self.adj_test = torch.from_numpy(self.adj_test).float().to(self.device)\n",
    "        # self.adj_all = torch.from_numpy(self.adj_all).float().to(self.device)\n",
    "\n",
    "        self.all_mean = torch.from_numpy(self.all_mean).float().to(self.device)\n",
    "        self.all_std = torch.from_numpy(self.all_std).float().to(self.device)\n",
    "\n",
    "        self.train_indices = torch.tensor(self.train_indices)\n",
    "        self.val_indices = torch.tensor(self.val_indices)\n",
    "        self.test_indices = torch.tensor(self.test_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3b9c2b-08ad-4330-9251-9637a071ab72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# 运行代码的主函数-这个是将缺失值视为变量的版本\n",
    "opt.model = 'TRAE_MVPT_MTL'\n",
    "# 遍历数据集名称\n",
    "for opt.data_name in ['wdbc']: #'iris', 'autompg', 'banknote', 'glass', 'seeds', 'pima', 'texture'\n",
    "    # 遍历不同的数据编号\n",
    "    for opt.id_num in [5,4,3,2]: # 1,2,3,4,5\n",
    "        # 遍历不同的缺失率\n",
    "        for opt.miss_rate in [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7]: #0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7\n",
    "            # 加载数据并进行预处理\n",
    "            DATA_ZZ = Dataset_zz(opt)\n",
    "            # 准备训练、验证和测试数据, 这里的是个人的数据导入\n",
    "            all_data_zz = Dataload_zz(DATA_ZZ.missing_data_all_norm, DATA_ZZ.all_data_norm, DATA_ZZ.all_label_onehot) \n",
    "            # train_data_zz = Dataload_zz(DATA_ZZ.missing_data_train_norm, DATA_ZZ.train_data_norm, DATA_ZZ.train_label_onehot, DATA_ZZ.adj_train) \n",
    "            # val_data_zz = Dataload_zz(DATA_ZZ.missing_data_val_norm, DATA_ZZ.val_data_norm, DATA_ZZ.val_label_onehot, DATA_ZZ.adj_val)\n",
    "            # test_data_zz = Dataload_zz(DATA_ZZ.missing_data_test_norm, DATA_ZZ.test_data_norm, DATA_ZZ.test_label_onehot, DATA_ZZ.adj_test)\n",
    "            global_train_idx, global_val_idx, global_test_idx = DATA_ZZ.train_indices, DATA_ZZ.val_indices, DATA_ZZ.test_indices\n",
    "\n",
    "            # 初始化模型\n",
    "            n_class = DATA_ZZ.train_label_onehot.shape[1] #不重复的类别数\n",
    "            global_mask_shape = DATA_ZZ.all_data.shape #训练集数据集/掩码矩阵形状，用于将缺失值视为变量动态优化\n",
    "            model = TRAE_MVPT_MTL(opt.n_hidden, n_class, global_mask_shape, DATA_ZZ, 'all').to(device) #构建模型\n",
    "            \n",
    "            # 加载预训练模型，如果指定了模型路径\n",
    "            if opt.load_model_path:\n",
    "                model.load(opt.load_model_path)\n",
    "            \n",
    "            # 准备数据加载器\n",
    "            all_dataloader = DataLoader(all_data_zz, opt.batch_size, shuffle=True, num_workers=opt.num_workers)\n",
    "            # train_dataloader = DataLoader(train_data_zz, opt.batch_size, shuffle=True, num_workers=opt.num_workers)\n",
    "            # val_dataloader = DataLoader(val_data_zz, opt.batch_size, shuffle=True, num_workers=opt.num_workers)\n",
    "            # test_dataloader = DataLoader(test_data_zz, opt.batch_size, shuffle=True, num_workers=opt.num_workers)\n",
    "            \n",
    "            # 定义损失函数和优化器\n",
    "            criterion_classicfic = torch.nn.NLLLoss().to(device)\n",
    "            criterion_imputation = torch.nn.MSELoss().to(device)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=opt.lr, weight_decay=opt.weight_decay)\n",
    "            \n",
    "            # # 初始化早停相关参数\n",
    "            # patience_acc, patience_mae = opt.patience_acc, opt.patience_mae  # 早停等待轮数\n",
    "            # best_acc = 0.0  # 目前为止最好的准确率\n",
    "            # best_mae = float('inf')  # 目前为止最低的MAE\n",
    "            # wait_acc = wait_mae = 0  # 早停等待计数器\n",
    "            # early_stop_triggered_acc = early_stop_triggered_mae = False # 标记是否早停\n",
    "            # test_performed_acc = test_performed_mae = False  # 标记是否已经执行了基于ACC和MAE的测试\n",
    "\n",
    "            # 初始化存储指标的列表\n",
    "            train_losses, val_losses, test_losses = [], [], []\n",
    "            train_maes, train_rmses, train_mapes, train_accs = [], [], [], []\n",
    "            val_maes, val_rmses, val_mapes, val_accs = [], [], [], []\n",
    "            test_maes, test_rmses, test_mapes, test_accs = [], [], [], []\n",
    "            train_mae_accs, val_mae_accs, test_mae_accs = [], [], []\n",
    "            train_de_maes, train_de_rmses, train_de_mapes = [], [], []\n",
    "            val_de_maes, val_de_rmses, val_de_mapes = [], [], []\n",
    "            test_de_maes, test_de_rmses, test_de_mapes = [], [], []\n",
    "\n",
    "\n",
    "            best_val_loss = float('inf')\n",
    "            patience, triggered = opt.patience, 0  # 早停的耐心值和触发计数\n",
    "            # 开始训练模型(验证和测试都在其中获取idx)\n",
    "            for epoch in range(opt.max_epoch):\n",
    "                # 重置每个epoch开始时的累积变量\n",
    "                epoch_input_data_list = []\n",
    "                epoch_imputed_data_list = []\n",
    "                epoch_target_data_list = [] #验证的时候是用真实值而不是缺失值变量的重构值\n",
    "                epoch_real_data_list = [] # 真实值\n",
    "                epoch_mask_list = [] #掩码矩阵\n",
    "                epoch_estimated_label_onehot_list = []\n",
    "                epoch_target_label_onehot_list = []\n",
    "                epoch_all_idx_list = []\n",
    "\n",
    "                for batch_data in all_dataloader: #按照批次取遍序号（全）\n",
    "                    # 解包数据\n",
    "                    batch_all_idx, batch_norm_missing_data, batch_norm_full_data, batch_label_onehot = batch_data\n",
    "                    batch_mask = ~torch.isnan(batch_norm_missing_data).to(device) # 将True/False反转，因为~操作符在bool张量上不工作\n",
    "                    \n",
    "                    # 输入模型前向传播，获取输出变量\n",
    "                    outputs = model(batch_norm_missing_data, batch_all_idx) #模型输出结果\n",
    "                    input_data, imputed_data, estimated_label_onehot = outputs # 解包模型输出\n",
    "                    target_data = batch_norm_full_data * batch_mask + input_data * (~batch_mask) # 目标训练结果是完整值的重构+缺失值变量的重构\n",
    "                    target_label_onehot = batch_label_onehot # 目标标签就是原始标签\n",
    "                    target_labels = torch.argmax(target_label_onehot, dim=1) # 获取分类标签\n",
    "\n",
    "                    # 提取出训练idx，验证idx，测试idx\n",
    "                    # 将全局索引转换为一个布尔掩码，用于标记批次中每个样本是否属于对应的集合\n",
    "                    mask_train_idx = torch.isin(batch_all_idx, global_train_idx)\n",
    "                    mask_val_idx = torch.isin(batch_all_idx, global_val_idx)\n",
    "                    mask_test_idx = torch.isin(batch_all_idx, global_test_idx)\n",
    "                    # 使用布尔掩码来选择当前批次中属于训练、验证和测试集的样本索引\n",
    "                    batch_train_idx = batch_all_idx[mask_train_idx]\n",
    "                    batch_val_idx = batch_all_idx[mask_val_idx]\n",
    "                    batch_test_idx = batch_all_idx[mask_test_idx]\n",
    "                    # 使用 torch.where 和布尔掩码获取在batch_all_idx中的相对位置\n",
    "                    batch_train_relative_idx = torch.where(mask_train_idx)\n",
    "                    batch_val_relative_idx = torch.where(mask_val_idx)\n",
    "                    batch_test_relative_idx = torch.where(mask_test_idx)\n",
    "\n",
    "                    # 填补损失，仅完整值的重构误差\n",
    "                    loss_imputation = criterion_imputation(imputed_data * batch_mask, target_data * batch_mask) # 重构误差+缺失值的变化误差@全数据集的@\n",
    "                    loss_imputation = loss_imputation/(1-opt.miss_rate)\n",
    "                    # 分类损失，仅训练集的\n",
    "                    loss_classicfic_train = criterion_classicfic(estimated_label_onehot[batch_train_relative_idx], target_labels[batch_train_relative_idx]) #分类误差使用训练集的分类误差\n",
    "                    # loss_classicfic_val = criterion_classicfic(estimated_label[batch_val_relative_idx], target_labels[batch_val_relative_idx]) #分类误差使用训练集的分类误差\n",
    "                    # loss_classicfic_test = criterion_classicfic(estimated_label[batch_test_relative_idx], target_labels[batch_test_relative_idx]) #分类误差使用训练集的分类误差\n",
    "                    # 总训练损失\n",
    "                    batch_model_loss_train = loss_classicfic_train + loss_imputation # 填补误差+分类误差\n",
    "                    # batch_model_loss_val = loss_classicfic_val + loss_imputation # 填补误差和分类误差各一半\n",
    "                    # batch_model_loss_test = loss_classicfic_test + loss_imputation # 填补误差和分类误差各一半\n",
    "                    \n",
    "                    # 优化\n",
    "                    optimizer.zero_grad()\n",
    "                    batch_model_loss_train.backward(retain_graph=True)\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # 在这里累积当前批次的相关变量\n",
    "                    epoch_input_data_list.append(input_data.detach())\n",
    "                    epoch_imputed_data_list.append(imputed_data.detach())\n",
    "                    epoch_real_data_list.append(batch_norm_full_data.detach()) # 真实值\n",
    "                    epoch_mask_list.append(batch_mask.detach()) #掩码矩阵\n",
    "                    epoch_target_data_list.append(target_data.detach())\n",
    "                    epoch_estimated_label_onehot_list.append(estimated_label_onehot.detach())\n",
    "                    epoch_target_label_onehot_list.append(target_label_onehot.detach())\n",
    "                    epoch_all_idx_list.append(batch_all_idx.detach())\n",
    "                ###################################################\n",
    "                #               验证和测试                        #\n",
    "                ###################################################\n",
    "                # 在epoch级别上累积数据\n",
    "                # （1）归一化填补结果，输入，目标值\n",
    "                epoch_input_data = torch.cat(epoch_input_data_list)\n",
    "                epoch_imputed_data = torch.cat(epoch_imputed_data_list)\n",
    "                epoch_target_data = torch.cat(epoch_target_data_list) #缺失变量的输入+有效值\n",
    "                epoch_real_data = torch.cat(epoch_real_data_list) # 真实值\n",
    "                epoch_mask = torch.cat(epoch_mask_list) #掩码矩阵\n",
    "                # （2）反归一化填补结果，输入，目标值\n",
    "                epoch_de_input_data = DATA_ZZ.denormalization_with_mean_std(epoch_input_data.clone(), DATA_ZZ.all_mean, DATA_ZZ.all_std)\n",
    "                epoch_de_imputed_data = DATA_ZZ.denormalization_with_mean_std(epoch_imputed_data.clone(), DATA_ZZ.all_mean, DATA_ZZ.all_std)\n",
    "                epoch_de_target_data = DATA_ZZ.denormalization_with_mean_std(epoch_target_data.clone(), DATA_ZZ.all_mean, DATA_ZZ.all_std) #缺失变量的输入+有效值\n",
    "                epoch_de_real_data = DATA_ZZ.denormalization_with_mean_std(epoch_real_data.clone(), DATA_ZZ.all_mean, DATA_ZZ.all_std) # 真实值\n",
    "                # （3）分类结果\n",
    "                epoch_estimated_label_onehot = torch.cat(epoch_estimated_label_onehot_list)\n",
    "                epoch_target_label_onehot = torch.cat(epoch_target_label_onehot_list)\n",
    "                epoch_all_idx = torch.cat(epoch_all_idx_list)\n",
    "                epoch_estimated_labels = torch.argmax(epoch_estimated_label_onehot, dim=1) # 获取分类标签\n",
    "                epoch_target_labels = torch.argmax(epoch_target_label_onehot, dim=1) # 获取分类标签\n",
    "\n",
    "                # 提取出训练idx，验证idx，测试idx\n",
    "                epoch_mask_train_idx = torch.isin(epoch_all_idx, global_train_idx)\n",
    "                epoch_mask_val_idx = torch.isin(epoch_all_idx, global_val_idx)\n",
    "                epoch_mask_test_idx = torch.isin(epoch_all_idx, global_test_idx)\n",
    "                # 使用布尔掩码来选择当前批次中属于训练、验证和测试集的样本索引\n",
    "                epoch_batch_train_idx = epoch_all_idx[epoch_mask_train_idx]\n",
    "                epoch_batch_val_idx = epoch_all_idx[epoch_mask_val_idx]\n",
    "                epoch_batch_test_idx = epoch_all_idx[epoch_mask_test_idx]\n",
    "                # 使用 torch.where 和布尔掩码获取在batch_all_idx中的相对位置\n",
    "                epoch_train_relative_idx = torch.where(epoch_mask_train_idx)\n",
    "                epoch_val_relative_idx = torch.where(epoch_mask_val_idx)\n",
    "                epoch_test_relative_idx = torch.where(epoch_mask_test_idx)\n",
    "\n",
    "                # 填补损失，仅完整值的重构误差\n",
    "                epoch_loss_imputation = criterion_imputation(epoch_input_data * epoch_mask, epoch_imputed_data * epoch_mask) # 重构误差+缺失值的变化误差@全数据集的@\n",
    "                epoch_loss_imputation = epoch_loss_imputation/(1-opt.miss_rate)\n",
    "                # 分类损失，仅训练集的\n",
    "                epoch_loss_classicfic_train = criterion_classicfic(epoch_estimated_label_onehot[epoch_train_relative_idx], epoch_target_labels[epoch_train_relative_idx]) #分类误差使用训练集的分类误差\n",
    "                epoch_loss_classicfic_val = criterion_classicfic(epoch_estimated_label_onehot[epoch_val_relative_idx], epoch_target_labels[epoch_val_relative_idx]) #分类误差使用训练集的分类误差\n",
    "                epoch_loss_classicfic_test = criterion_classicfic(epoch_estimated_label_onehot[epoch_test_relative_idx], epoch_target_labels[epoch_test_relative_idx]) #分类误差使用训练集的分类误差\n",
    "                # 总损失\n",
    "                epoch_model_loss_train = epoch_loss_classicfic_train + epoch_loss_imputation # 填补误差+分类误差\n",
    "                epoch_model_loss_val = epoch_loss_classicfic_val + epoch_loss_imputation # 填补误差和分类误差各一半\n",
    "                epoch_model_loss_test = epoch_loss_classicfic_test + epoch_loss_imputation # 填补误差和分类误差各一半\n",
    "                \n",
    "                # 评价指标,这个地方需要重新设计，究竟是用重构误差还是怎么着\n",
    "                # （1）归一化误差\n",
    "                # 填补误差，这里是重构误差\n",
    "                epoch_mae_train, epoch_rmse_train, epoch_mape_train = calculate_imputation_metrics(epoch_imputed_data[epoch_train_relative_idx], epoch_target_data[epoch_train_relative_idx])\n",
    "                # 验证误差，这里是重构误差，因为真值不知道，可用于中间过程早停\n",
    "                epoch_mae_val, epoch_rmse_val, epoch_mape_val = calculate_imputation_metrics(epoch_imputed_data[epoch_val_relative_idx], epoch_target_data[epoch_val_relative_idx])\n",
    "                # 填补误差，这里用真值，仅用于最后的结果\n",
    "                epoch_mae_test, epoch_rmse_test, epoch_mape_test = calculate_imputation_metrics_mask(epoch_imputed_data[epoch_test_relative_idx], epoch_real_data[epoch_test_relative_idx], epoch_mask[epoch_test_relative_idx])\n",
    "                \n",
    "                # （2）反归一化误差\n",
    "                # 填补误差，这里是重构误差\n",
    "                epoch_de_mae_train, epoch_de_rmse_train, epoch_de_mape_train = calculate_imputation_metrics(epoch_de_imputed_data[epoch_train_relative_idx], epoch_de_target_data[epoch_train_relative_idx])\n",
    "                # 验证误差，这里是重构误差，因为真值不知道，可用于中间过程早停\n",
    "                epoch_de_mae_val, epoch_de_rmse_val, epoch_de_mape_val = calculate_imputation_metrics(epoch_de_imputed_data[epoch_val_relative_idx], epoch_de_target_data[epoch_val_relative_idx])\n",
    "                # 填补误差，这里用真值，仅用于最后的结果\n",
    "                epoch_de_mae_test, epoch_de_rmse_test, epoch_de_mape_test = calculate_imputation_metrics_mask(epoch_de_imputed_data[epoch_test_relative_idx], epoch_de_real_data[epoch_test_relative_idx], epoch_mask[epoch_test_relative_idx])\n",
    "                \n",
    "                # （3）分类误差\n",
    "                # 分类准确率，使用epoch_estimated_label和epoch_target_labels计算\n",
    "                epoch_acc_train = calculate_accuracy(epoch_estimated_labels[epoch_train_relative_idx], epoch_target_labels[epoch_train_relative_idx])\n",
    "                epoch_acc_val = calculate_accuracy(epoch_estimated_labels[epoch_val_relative_idx], epoch_target_labels[epoch_val_relative_idx])\n",
    "                epoch_acc_test = calculate_accuracy(epoch_estimated_labels[epoch_test_relative_idx], epoch_target_labels[epoch_test_relative_idx])\n",
    "\n",
    "                # 记录指标\n",
    "                train_losses.append(epoch_model_loss_train.item())\n",
    "                val_losses.append(epoch_model_loss_val.item())\n",
    "                test_losses.append(epoch_model_loss_test.item())\n",
    "                train_maes.append(epoch_mae_train)\n",
    "                train_rmses.append(epoch_rmse_train)\n",
    "                train_mapes.append(epoch_mape_train)\n",
    "                train_accs.append(epoch_acc_train)\n",
    "                val_maes.append(epoch_mae_val)\n",
    "                val_rmses.append(epoch_rmse_val)\n",
    "                val_mapes.append(epoch_mape_val)\n",
    "                val_accs.append(epoch_acc_val)\n",
    "                test_maes.append(epoch_mae_test)\n",
    "                test_rmses.append(epoch_rmse_test)\n",
    "                test_mapes.append(epoch_mape_test)\n",
    "                test_accs.append(epoch_acc_test)\n",
    "                train_de_maes.append(epoch_de_mae_train)\n",
    "                train_de_rmses.append(epoch_de_rmse_train)\n",
    "                train_de_mapes.append(epoch_de_mape_train)\n",
    "                val_de_maes.append(epoch_de_mae_val)\n",
    "                val_de_rmses.append(epoch_de_rmse_val)\n",
    "                val_de_mapes.append(epoch_de_mape_val)\n",
    "                test_de_maes.append(epoch_de_mae_test)\n",
    "                test_de_rmses.append(epoch_de_rmse_test)\n",
    "                test_de_mapes.append(epoch_de_mape_test)\n",
    "\n",
    "                # 在每个epoch的末尾计算新指标\n",
    "                train_mae_acc = epoch_mae_train / epoch_acc_train\n",
    "                val_mae_acc = epoch_mae_val / epoch_acc_val\n",
    "                test_mae_acc = epoch_mae_test / epoch_acc_test\n",
    "\n",
    "                # 将新指标添加到列表中\n",
    "                train_mae_accs.append(train_mae_acc)\n",
    "                val_mae_accs.append(val_mae_acc)\n",
    "                test_mae_accs.append(test_mae_acc)\n",
    "\n",
    "                # # 更新早停条件，使用val_mae_acc作为参考\n",
    "                # if val_mae_acc < best_val_loss:  # 注意这里应根据你的指标性质决定比较方向\n",
    "                #     best_val_loss = val_mae_acc\n",
    "                #     triggered = 0\n",
    "                # else:\n",
    "                #     triggered += 1\n",
    "                #     if triggered >= patience:\n",
    "                #         print(\"Early stopping triggered at epoch {}\".format(epoch))\n",
    "                #         break\n",
    "\n",
    "                # 检查早停条件\n",
    "                if epoch_model_loss_val < best_val_loss:\n",
    "                    best_val_loss = epoch_model_loss_val\n",
    "                    triggered = 0\n",
    "                else:\n",
    "                    triggered += 1\n",
    "                    if triggered >= patience:\n",
    "                        print(\"Early stopping triggered at epoch {}\".format(epoch))\n",
    "                        break\n",
    "            \n",
    "            # 保存最优结果\n",
    "            # 找到最小验证集损失对应的指标\n",
    "            min_val_loss_index = val_losses.index(min(val_losses))\n",
    "\n",
    "            # 构建一行数据记录\n",
    "            data_record = {\n",
    "                'Dataset': opt.data_name,\n",
    "                'Missing Rate':opt.miss_rate,\n",
    "                'Data ID':opt.id_num,\n",
    "                'Model':opt.model,\n",
    "                'Epoch': min_val_loss_index + 1,\n",
    "\n",
    "                'Train Loss': train_losses[min_val_loss_index],\n",
    "                'Validation Loss': val_losses[min_val_loss_index],\n",
    "                'Test Loss': test_losses[min_val_loss_index],\n",
    "\n",
    "                'Train Accuracy': train_accs[min_val_loss_index]*100,\n",
    "                'Val Accuracy': val_accs[min_val_loss_index]*100,\n",
    "                'Test Accuracy': test_accs[min_val_loss_index]*100,\n",
    "\n",
    "                'Train MAE(reconstruct)': train_maes[min_val_loss_index],\n",
    "                'Val MAE(reconstruct)': val_maes[min_val_loss_index],\n",
    "                'Test MAE': test_maes[min_val_loss_index],\n",
    "\n",
    "                'Train RMSE(reconstruct)': train_rmses[min_val_loss_index],\n",
    "                'Val RMSE(reconstruct)': val_rmses[min_val_loss_index],\n",
    "                'Test RMSE': test_rmses[min_val_loss_index],\n",
    "\n",
    "                'Train MAPE(reconstruct)': train_mapes[min_val_loss_index],\n",
    "                'Val MAPE(reconstruct)': val_mapes[min_val_loss_index],\n",
    "                'Test MAPE': test_mapes[min_val_loss_index],\n",
    "\n",
    "                'Train de_MAE(reconstruct)': train_de_maes[min_val_loss_index],\n",
    "                'Val de_MAE(reconstruct)': val_de_maes[min_val_loss_index],\n",
    "                'Test de_MAE': test_de_maes[min_val_loss_index],\n",
    "\n",
    "                'Train de_RMSE(reconstruct)': train_de_rmses[min_val_loss_index],\n",
    "                'Val de_RMSE(reconstruct)': val_de_rmses[min_val_loss_index],\n",
    "                'Test de_RMSE': test_de_rmses[min_val_loss_index],\n",
    "\n",
    "                'Train de_MAPE(reconstruct)': train_de_mapes[min_val_loss_index],\n",
    "                'Val de_MAPE(reconstruct)': val_de_mapes[min_val_loss_index],\n",
    "                'Test de_MAPE': test_de_mapes[min_val_loss_index],\n",
    "                # 'Train MAE*ACC': train_mae_accs[min_val_loss_index],\n",
    "                # 'Val MAE*ACC': val_mae_accs[min_val_loss_index],\n",
    "                # 'Test MAE*ACC': test_mae_accs[min_val_loss_index]\n",
    "            }\n",
    "\n",
    "            #### 保存到汇总结果处\n",
    "            # 总文件路径\n",
    "            filename = f'zz_result/results.csv'\n",
    "\n",
    "            # 检查文件是否存在，不存在则创建\n",
    "            if not os.path.isfile(filename):\n",
    "                # 创建DataFrame，并将数据记录为第一行\n",
    "                df = pd.DataFrame([data_record])\n",
    "            else:\n",
    "                # 如果文件已存在，读取文件\n",
    "                df = pd.read_csv(filename)\n",
    "                # 添加新的记录\n",
    "                df = df.append(data_record, ignore_index=True)\n",
    "\n",
    "            # 保存到CSV文件\n",
    "            df.to_csv(filename, index=False)\n",
    "\n",
    "            #### 保存到按照数据集划分的文件夹，方便后续数据分析，请调用实验数据处理.ipynb\n",
    "            # 分文件路径\n",
    "            filename = f'zz_result/{opt.data_name}/{opt.data_name}.csv'\n",
    "\n",
    "            # 检查文件是否存在，不存在则创建\n",
    "            if not os.path.isfile(filename):\n",
    "                # 创建DataFrame，并将数据记录为第一行\n",
    "                df = pd.DataFrame([data_record])\n",
    "            else:\n",
    "                # 如果文件已存在，读取文件\n",
    "                df = pd.read_csv(filename)\n",
    "                # 添加新的记录\n",
    "                df = df.append(data_record, ignore_index=True)\n",
    "\n",
    "            # 保存到CSV文件\n",
    "            df.to_csv(filename, index=False)\n",
    "\n",
    "            # # 开始绘图\n",
    "            # # 绘制损失曲线\n",
    "            # plt.figure(figsize=(15, 9))\n",
    "            # plt.subplot(3, 3, 1)\n",
    "            # plt.plot(train_losses, label='Train Loss')\n",
    "            # plt.plot(val_losses, label='Validation Loss')\n",
    "            # plt.plot(test_losses, label='Test Loss')\n",
    "            # plt.title('Loss over epochs')\n",
    "            # plt.xlabel('Epoch')\n",
    "            # plt.ylabel('Loss')\n",
    "            # plt.legend()\n",
    "\n",
    "            # # 绘制评价指标曲线\n",
    "            # plt.subplot(3, 3, 2)\n",
    "            # plt.plot(train_maes, label='Train MAE')\n",
    "            # plt.plot(val_maes, label='Val MAE')\n",
    "            # plt.plot(test_maes, label='Test MAE')\n",
    "            # plt.title('Metrics over epochs')\n",
    "            # plt.xlabel('Epoch')\n",
    "            # plt.legend()\n",
    "\n",
    "            # # 绘制评价指标曲线\n",
    "            # plt.subplot(3, 3, 3)\n",
    "            # plt.plot(train_de_maes, label='Train de_MAE')\n",
    "            # plt.plot(val_de_maes, label='Val de_MAE')\n",
    "            # plt.plot(test_de_maes, label='Test de_MAE')\n",
    "            # plt.title('Metrics over epochs')\n",
    "            # plt.xlabel('Epoch')\n",
    "            # plt.legend()\n",
    "\n",
    "            # # 绘制评价指标曲线\n",
    "            # plt.subplot(3, 3, 4)\n",
    "            # plt.plot(train_rmses, label='Train RMSE')\n",
    "            # plt.plot(val_rmses, label='Val RMSE')\n",
    "            # plt.plot(test_rmses, label='Test RMSE')\n",
    "            # plt.title('Metrics over epochs')\n",
    "            # plt.xlabel('Epoch')\n",
    "            # plt.legend()\n",
    "\n",
    "            # # 绘制评价指标曲线\n",
    "            # plt.subplot(3, 3, 5)\n",
    "            # plt.plot(train_de_rmses, label='Train de_RMSE')\n",
    "            # plt.plot(val_de_rmses, label='Val de_RMSE')\n",
    "            # plt.plot(test_de_rmses, label='Test de_RMSE')\n",
    "            # plt.title('Metrics over epochs')\n",
    "            # plt.xlabel('Epoch')\n",
    "            # plt.legend()\n",
    "\n",
    "            # # 绘制评价指标曲线\n",
    "            # plt.subplot(3, 3, 6)\n",
    "            # plt.plot(train_mapes, label='Train MAPE')\n",
    "            # plt.plot(val_mapes, label='Val MAPE')\n",
    "            # plt.plot(test_mapes, label='Test MAPE')\n",
    "            # plt.title('Metrics over epochs')\n",
    "            # plt.xlabel('Epoch')\n",
    "            # plt.legend()\n",
    "\n",
    "            # # 绘制评价指标曲线\n",
    "            # plt.subplot(3, 3, 7)\n",
    "            # plt.plot(train_de_mapes, label='Train de_MAPE')\n",
    "            # plt.plot(val_de_mapes, label='Val de_MAPE')\n",
    "            # plt.plot(test_de_mapes, label='Test de_MAPE')\n",
    "            # plt.title('Metrics over epochs')\n",
    "            # plt.xlabel('Epoch')\n",
    "            # plt.legend()\n",
    "\n",
    "            # # 绘制评价指标曲线\n",
    "            # plt.subplot(3, 3, 8)\n",
    "            # plt.plot(train_accs, label='Train Accuracy')\n",
    "            # plt.plot(val_accs, label='Val Accuracy')\n",
    "            # plt.plot(test_accs, label='Test Accuracy')\n",
    "            # plt.title('Metrics over epochs')\n",
    "            # plt.xlabel('Epoch')\n",
    "            # plt.legend()\n",
    "\n",
    "            # # 绘制MAE*ACC指标曲线\n",
    "            # plt.subplot(3, 3, 9)  # 假设你已有3个图表，这是第4个\n",
    "            # plt.plot(train_mae_accs, label='Train MAE/ACC')\n",
    "            # plt.plot(val_mae_accs, label='Validation MAE/ACC')\n",
    "            # plt.plot(test_mae_accs, label='Test MAE/ACC')\n",
    "            # plt.title('MAE*ACC over epochs')\n",
    "            # plt.xlabel('Epoch')\n",
    "            # plt.ylabel('MAE*ACC')\n",
    "            # plt.legend()\n",
    "\n",
    "            # plt.tight_layout()\n",
    "            # plt.show()\n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "    #             print(epoch_acc_test)\n",
    "\n",
    "\n",
    "    #             val_rmse, val_mae, val_mape, val_rmse_norm, val_mae_norm, val_mape_norm, val_acc = evaluate(opt, model, val_dataloader, feature_label_mask_miss, adj_euclidean, adj_mahalanobis, real_features=features_ori, norm_real_features=norm_features_ori, max_zz=max_j_zz, min_zz=min_j_zz, data_mask_add=data_mask_add)\n",
    "    #             model.train()  # 确保模型处于训练模式\n",
    "    #             if epoch % 1000 == 0 :\n",
    "    #                 print(f\"第{epoch}轮训练后，验证集的ACC为：{(val_acc*100):.2f}%,val_rmse:{val_rmse:.2f}, val_mae:{val_mae:.2f}, val_mape:{val_mape:.2f}, val_rmse_norm:{val_rmse_norm:.2f}, val_mae_norm:{val_mae_norm:.2f}, val_mape_norm:{val_mape_norm:.2f}\")\n",
    "    # # ######################### 新版本早停  #############################\n",
    "    #             # 更新ACC早停逻辑\n",
    "    #             if val_acc > best_acc:\n",
    "    #                 best_acc = val_acc\n",
    "    #                 wait_acc = 0\n",
    "    #                 # 保存当前最优模型状态 - 基于ACC\n",
    "    #                 torch.save(model.state_dict(), opt.model_save_path_acc)\n",
    "    #             else:\n",
    "    #                 wait_acc += 1\n",
    "    #                 if wait_acc >= patience_acc and not test_performed_acc:\n",
    "    #                     early_stop_triggered_acc = True\n",
    "    #                     print(f'模型按ACC标准早停, 运行轮数为{epoch}, 最好的验证集分类精度为: {best_acc*100:.2f}%')\n",
    "    #                     # 执行基于ACC的测试并记录结果\n",
    "    #                     test_performed_acc = True\n",
    "\n",
    "    #             # 更新MAE早停逻辑\n",
    "    #             if val_mae < best_mae:\n",
    "    #                 best_mae = val_mae \n",
    "    #                 wait_mae = 0\n",
    "    #                 # 保存当前最优模型状态 - 基于MAE\n",
    "    #                 torch.save(model.state_dict(), opt.model_save_path_mae)\n",
    "    #             else:\n",
    "    #                 wait_mae += 1\n",
    "    #                 if wait_mae >= patience_mae and not test_performed_mae:\n",
    "    #                     early_stop_triggered_mae = True\n",
    "    #                     print(f'模型按MAE标准早停, 运行轮数为{epoch}, 最低的验证集MAE为: {best_mae:.2f}')\n",
    "    #                     # 执行基于MAE的测试并记录结果\n",
    "    #                     test_performed_mae = True\n",
    "\n",
    "    #             # 检查是否已经触发了基于两个指标的早停并执行了测试\n",
    "    #             if test_performed_acc and test_performed_mae:\n",
    "    #                 print(\"基于ACC和MAE的早停条件都已满足，训练结束。\")\n",
    "    #                 print(\"#####\"*10)\n",
    "    #                 break\n",
    "\n",
    "    #         # 测试阶段，调用保存的模型状态进行\n",
    "\n",
    "    #         # 根据ACC的最优模型状态进行测试\n",
    "    #         print(\"加载基于ACC的最优模型状态进行测试...\")\n",
    "    #         model.load_state_dict(torch.load(opt.model_save_path_acc))\n",
    "    #         # 在测试集上评估模型，寻找最佳性能\n",
    "    #         rmse, mae, mape, rmse_norm, mae_norm, mape_norm, acc = evaluate(opt, model, test_dataloader, feature_label_mask_miss, adj_euclidean, adj_mahalanobis, real_features=features_ori, norm_real_features=norm_features_ori, max_zz=max_j_zz, min_zz=min_j_zz, data_mask_add=data_mask_add)\n",
    "    #         print(f\"在测试集上的mae是{mae:.2f}, mape是{mape:.2f}, acc是{acc*100:.0f}%, norm_mae是{mae_norm:.2f}\")\n",
    "    #         # 记录测试结果\n",
    "    #         writeline_csv(\"zz_result/record_acc.csv\", rmse, mae, mape, acc, rmse_norm, mae_norm, mape_norm)\n",
    "\n",
    "    #         # 根据MAE的最优模型状态进行测试\n",
    "    #         print(\"加载基于MAE的最优模型状态进行测试...\")\n",
    "    #         model.load_state_dict(torch.load(opt.model_save_path_mae))\n",
    "    #         # 在测试集上评估模型，寻找最佳性能\n",
    "    #         rmse, mae, mape, rmse_norm, mae_norm, mape_norm, acc = evaluate(opt, model, test_dataloader, feature_label_mask_miss, adj_euclidean, adj_mahalanobis, real_features=features_ori, norm_real_features=norm_features_ori, max_zz=max_j_zz, min_zz=min_j_zz, data_mask_add=data_mask_add)\n",
    "    #         print(f\"在测试集上的mae是{mae:.2f}, mape是{mape:.2f}, acc是{acc*100:.0f}%, norm_mae是{mae_norm:.2f}\")\n",
    "    #         print(\"-----\"*10)\n",
    "    #         # 记录测试结果\n",
    "    #         writeline_csv(\"zz_result/record_mae.csv\", rmse, mae, mape, acc, rmse_norm, mae_norm, mape_norm)\n",
    "    # # ######################### 旧版本早停  #############################甚至没有保存模型状态\n",
    "    # #             # 检查是否需要早停\n",
    "    # #             if val_acc > best_acc:\n",
    "    # #                 best_acc = val_acc\n",
    "    # #                 wait = 0  # 重置等待计数器\n",
    "    # #             else:\n",
    "    # #                 wait += 1\n",
    "    # #                 if wait >= patience:\n",
    "    # #                     print(f'模型早停, 运行轮数为{epoch}, 最好的验证集分类精度为: {best_acc*100:.2f}%')\n",
    "    # #                     break  # 早停\n",
    "    # #         # 在测试集上评估模型，寻找最佳性能\n",
    "    # #         rmse, mae, mape, rmse_norm, mae_norm, mape_norm, acc = evaluate(opt, model, test_dataloader, feature_label_mask_miss, adj_euclidean, adj_mahalanobis, real_features=features_ori, norm_real_features=norm_features_ori, max_zz=max_j_zz, min_zz=min_j_zz, data_mask_add=data_mask_add)\n",
    "    # #         print(f\"在测试集上，最终的mae是{mae:.2f}, mape是{mape:.2f}, acc是{acc*100:.0f}%, norm_mae是{mae_norm:.2f}\")\n",
    "    # #         print(\"-----\"*10)\n",
    "    # #         # 记录测试结果\n",
    "    # #         writeline_csv(\"zz_result/record.csv\", rmse, mae, mape, acc, rmse_norm, mae_norm, mape_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383e989e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
